{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are hired by NASA as a Data Scientist, Congratulations!\n",
    "#your first project;  you have to classify stars based on their astornomical measurements,  0 non-pulsar, 1-pulsar\n",
    "#luckily astronouts gave you a dataset after making observations through our Galaxy\n",
    "#your job is to use ML techniques as SVM, Decision_tree, Random_forest and Kneighbours\n",
    "#which ML model gives the best?\n",
    "#Because you are working in NASA, they have very clean dataset(there is no preprocessing!)\n",
    "#to learn more about pulsars\n",
    "#you will find your dataset inside the rar file named as \"pulsar_stars.csv\"\n",
    "#  https://www.nasa.gov/mission_pages/GLAST/science/neutron_stars.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean_of_the_integrated_profile</th>\n",
       "      <th>Standard_deviation_of_the_integrated_profile</th>\n",
       "      <th>Excess_kurtosis_of_the_integrated_profile</th>\n",
       "      <th>Skewness_of_the_integrated_profile</th>\n",
       "      <th>Mean_of_the_DM_SNR_curve</th>\n",
       "      <th>Standard_deviation_of_the_DM_SNR_curve</th>\n",
       "      <th>Excess_kurtosis_of_the_DM_SNR_curve</th>\n",
       "      <th>Skewness_of_the_DM_SNR_curve</th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17888</th>\n",
       "      <td>98.726562</td>\n",
       "      <td>50.407823</td>\n",
       "      <td>0.565124</td>\n",
       "      <td>0.245231</td>\n",
       "      <td>0.570234</td>\n",
       "      <td>9.011285</td>\n",
       "      <td>22.018589</td>\n",
       "      <td>561.833787</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17889</th>\n",
       "      <td>126.625000</td>\n",
       "      <td>55.721826</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>-0.303218</td>\n",
       "      <td>0.534281</td>\n",
       "      <td>8.588882</td>\n",
       "      <td>23.913761</td>\n",
       "      <td>660.197035</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17890</th>\n",
       "      <td>143.671875</td>\n",
       "      <td>45.302647</td>\n",
       "      <td>-0.045769</td>\n",
       "      <td>0.353643</td>\n",
       "      <td>5.173913</td>\n",
       "      <td>26.462345</td>\n",
       "      <td>5.706651</td>\n",
       "      <td>33.802613</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17891</th>\n",
       "      <td>118.484375</td>\n",
       "      <td>50.608483</td>\n",
       "      <td>-0.029059</td>\n",
       "      <td>-0.027494</td>\n",
       "      <td>0.422241</td>\n",
       "      <td>8.086684</td>\n",
       "      <td>27.446113</td>\n",
       "      <td>830.638550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17892</th>\n",
       "      <td>96.000000</td>\n",
       "      <td>44.193113</td>\n",
       "      <td>0.388674</td>\n",
       "      <td>0.281344</td>\n",
       "      <td>1.871237</td>\n",
       "      <td>15.833746</td>\n",
       "      <td>9.634927</td>\n",
       "      <td>104.821623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17893</th>\n",
       "      <td>136.429688</td>\n",
       "      <td>59.847421</td>\n",
       "      <td>-0.187846</td>\n",
       "      <td>-0.738123</td>\n",
       "      <td>1.296823</td>\n",
       "      <td>12.166062</td>\n",
       "      <td>15.450260</td>\n",
       "      <td>285.931022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17894</th>\n",
       "      <td>122.554688</td>\n",
       "      <td>49.485605</td>\n",
       "      <td>0.127978</td>\n",
       "      <td>0.323061</td>\n",
       "      <td>16.409699</td>\n",
       "      <td>44.626893</td>\n",
       "      <td>2.945244</td>\n",
       "      <td>8.297092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17895</th>\n",
       "      <td>119.335938</td>\n",
       "      <td>59.935939</td>\n",
       "      <td>0.159363</td>\n",
       "      <td>-0.743025</td>\n",
       "      <td>21.430602</td>\n",
       "      <td>58.872000</td>\n",
       "      <td>2.499517</td>\n",
       "      <td>4.595173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17896</th>\n",
       "      <td>114.507812</td>\n",
       "      <td>53.902400</td>\n",
       "      <td>0.201161</td>\n",
       "      <td>-0.024789</td>\n",
       "      <td>1.946488</td>\n",
       "      <td>13.381731</td>\n",
       "      <td>10.007967</td>\n",
       "      <td>134.238910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17897</th>\n",
       "      <td>57.062500</td>\n",
       "      <td>85.797340</td>\n",
       "      <td>1.406391</td>\n",
       "      <td>0.089520</td>\n",
       "      <td>188.306020</td>\n",
       "      <td>64.712562</td>\n",
       "      <td>-1.597527</td>\n",
       "      <td>1.429475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Mean_of_the_integrated_profile  \\\n",
       "17888                       98.726562   \n",
       "17889                      126.625000   \n",
       "17890                      143.671875   \n",
       "17891                      118.484375   \n",
       "17892                       96.000000   \n",
       "17893                      136.429688   \n",
       "17894                      122.554688   \n",
       "17895                      119.335938   \n",
       "17896                      114.507812   \n",
       "17897                       57.062500   \n",
       "\n",
       "       Standard_deviation_of_the_integrated_profile  \\\n",
       "17888                                     50.407823   \n",
       "17889                                     55.721826   \n",
       "17890                                     45.302647   \n",
       "17891                                     50.608483   \n",
       "17892                                     44.193113   \n",
       "17893                                     59.847421   \n",
       "17894                                     49.485605   \n",
       "17895                                     59.935939   \n",
       "17896                                     53.902400   \n",
       "17897                                     85.797340   \n",
       "\n",
       "       Excess_kurtosis_of_the_integrated_profile  \\\n",
       "17888                                   0.565124   \n",
       "17889                                   0.002946   \n",
       "17890                                  -0.045769   \n",
       "17891                                  -0.029059   \n",
       "17892                                   0.388674   \n",
       "17893                                  -0.187846   \n",
       "17894                                   0.127978   \n",
       "17895                                   0.159363   \n",
       "17896                                   0.201161   \n",
       "17897                                   1.406391   \n",
       "\n",
       "       Skewness_of_the_integrated_profile  Mean_of_the_DM_SNR_curve  \\\n",
       "17888                            0.245231                  0.570234   \n",
       "17889                           -0.303218                  0.534281   \n",
       "17890                            0.353643                  5.173913   \n",
       "17891                           -0.027494                  0.422241   \n",
       "17892                            0.281344                  1.871237   \n",
       "17893                           -0.738123                  1.296823   \n",
       "17894                            0.323061                 16.409699   \n",
       "17895                           -0.743025                 21.430602   \n",
       "17896                           -0.024789                  1.946488   \n",
       "17897                            0.089520                188.306020   \n",
       "\n",
       "       Standard_deviation_of_the_DM_SNR_curve  \\\n",
       "17888                                9.011285   \n",
       "17889                                8.588882   \n",
       "17890                               26.462345   \n",
       "17891                                8.086684   \n",
       "17892                               15.833746   \n",
       "17893                               12.166062   \n",
       "17894                               44.626893   \n",
       "17895                               58.872000   \n",
       "17896                               13.381731   \n",
       "17897                               64.712562   \n",
       "\n",
       "        Excess_kurtosis_of_the_DM_SNR_curve   Skewness_of_the_DM_SNR_curve  \\\n",
       "17888                             22.018589                     561.833787   \n",
       "17889                             23.913761                     660.197035   \n",
       "17890                              5.706651                      33.802613   \n",
       "17891                             27.446113                     830.638550   \n",
       "17892                              9.634927                     104.821623   \n",
       "17893                             15.450260                     285.931022   \n",
       "17894                              2.945244                       8.297092   \n",
       "17895                              2.499517                       4.595173   \n",
       "17896                             10.007967                     134.238910   \n",
       "17897                             -1.597527                       1.429475   \n",
       "\n",
       "       target_class  \n",
       "17888             0  \n",
       "17889             0  \n",
       "17890             0  \n",
       "17891             0  \n",
       "17892             0  \n",
       "17893             0  \n",
       "17894             0  \n",
       "17895             0  \n",
       "17896             0  \n",
       "17897             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "StarsDataset = pd.read_csv('pulsar_stars.csv')\n",
    "StarsDataset.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mean_of_the_integrated_profile                  0\n",
       "Standard_deviation_of_the_integrated_profile    0\n",
       "Excess_kurtosis_of_the_integrated_profile       0\n",
       "Skewness_of_the_integrated_profile              0\n",
       "Mean_of_the_DM_SNR_curve                        0\n",
       "Standard_deviation_of_the_DM_SNR_curve          0\n",
       " Excess_kurtosis_of_the_DM_SNR_curve            0\n",
       " Skewness_of_the_DM_SNR_curve                   0\n",
       "target_class                                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StarsDataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = StarsDataset.drop('target_class', axis=1)\n",
    "y = StarsDataset['target_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train\n",
      "1352     0\n",
      "9109     0\n",
      "11365    0\n",
      "Name: target_class, dtype: int64\n",
      "X_test\n",
      "      Mean_of_the_integrated_profile  \\\n",
      "5253                      120.437500   \n",
      "7719                       23.804688   \n",
      "9747                      128.601562   \n",
      "\n",
      "      Standard_deviation_of_the_integrated_profile  \\\n",
      "5253                                     52.428486   \n",
      "7719                                     41.677905   \n",
      "9747                                     43.864554   \n",
      "\n",
      "      Excess_kurtosis_of_the_integrated_profile  \\\n",
      "5253                                  -0.098114   \n",
      "7719                                   3.938470   \n",
      "9747                                   0.252631   \n",
      "\n",
      "      Skewness_of_the_integrated_profile  Mean_of_the_DM_SNR_curve  \\\n",
      "5253                           -0.386378                  3.379599   \n",
      "7719                           15.420309                108.270903   \n",
      "9747                            0.303181                 48.479097   \n",
      "\n",
      "      Standard_deviation_of_the_DM_SNR_curve  \\\n",
      "5253                               17.183392   \n",
      "7719                               53.155749   \n",
      "9747                               81.770186   \n",
      "\n",
      "       Excess_kurtosis_of_the_DM_SNR_curve   Skewness_of_the_DM_SNR_curve  \n",
      "5253                              7.663754                      76.443913  \n",
      "7719                              0.759664                       0.346260  \n",
      "9747                              1.231203                      -0.245005  \n",
      "y_test\n",
      "5253    0\n",
      "7719    1\n",
      "9747    0\n",
      "Name: target_class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y, test_size = 0.2, shuffle = True)\n",
    "# print(\"X_train\")\n",
    "# print(X_train.head(3))\n",
    "print(\"y_train\")\n",
    "print(y_train.head(3))\n",
    "\n",
    "print(\"X_test\")\n",
    "print(X_test.head(3))\n",
    "print(\"y_test\")\n",
    "print(y_test.head(3))\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Mean_of_the_integrated_profile  \\\n",
      "1352                         0.519970   \n",
      "9109                         0.474635   \n",
      "11365                        0.555226   \n",
      "\n",
      "       Standard_deviation_of_the_integrated_profile  \\\n",
      "1352                                       0.216129   \n",
      "9109                                       0.322513   \n",
      "11365                                      0.281511   \n",
      "\n",
      "       Excess_kurtosis_of_the_integrated_profile  \\\n",
      "1352                                    0.243580   \n",
      "9109                                    0.241792   \n",
      "11365                                   0.208907   \n",
      "\n",
      "       Skewness_of_the_integrated_profile  Mean_of_the_DM_SNR_curve  \\\n",
      "1352                             0.043901                  0.018743   \n",
      "9109                             0.032949                  0.075857   \n",
      "11365                            0.036961                  0.006811   \n",
      "\n",
      "       Standard_deviation_of_the_DM_SNR_curve  \\\n",
      "1352                                 0.149367   \n",
      "9109                                 0.340561   \n",
      "11365                                0.050052   \n",
      "\n",
      "        Excess_kurtosis_of_the_DM_SNR_curve   Skewness_of_the_DM_SNR_curve  \n",
      "1352                               0.241902                       0.034817  \n",
      "9109                               0.154377                       0.007736  \n",
      "11365                              0.378938                       0.142257  \n"
     ]
    }
   ],
   "source": [
    "X_train = pd.DataFrame(data=scaler.transform(X_train),columns = X_train.columns,index=X_train.index)\n",
    "X_test = pd.DataFrame(data=scaler.transform(X_test),columns = X_test.columns,index=X_test.index)\n",
    "print(X_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "y_pred = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3246   14]\n",
      " [  73  247]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      3260\n",
      "           1       0.95      0.77      0.85       320\n",
      "\n",
      "    accuracy                           0.98      3580\n",
      "   macro avg       0.96      0.88      0.92      3580\n",
      "weighted avg       0.98      0.98      0.97      3580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "SVC_score = precision_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3195   65]\n",
      " [  56  264]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      3260\n",
      "           1       0.80      0.82      0.81       320\n",
      "\n",
      "    accuracy                           0.97      3580\n",
      "   macro avg       0.89      0.90      0.90      3580\n",
      "weighted avg       0.97      0.97      0.97      3580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train,y_train)\n",
    "tree_pred = decision_tree.predict(X_test)\n",
    "print(confusion_matrix(y_test,tree_pred))\n",
    "print(classification_report(y_test,tree_pred))\n",
    "DTC_score = precision_score(y_test,tree_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3235   25]\n",
      " [  49  271]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3260\n",
      "           1       0.92      0.85      0.88       320\n",
      "\n",
      "    accuracy                           0.98      3580\n",
      "   macro avg       0.95      0.92      0.93      3580\n",
      "weighted avg       0.98      0.98      0.98      3580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "print(confusion_matrix(y_test,rf_pred))\n",
    "print(classification_report(y_test,rf_pred))\n",
    "RFC_score = precision_score(y_test,rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3235   25]\n",
      " [  51  269]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      3260\n",
      "           1       0.91      0.84      0.88       320\n",
      "\n",
      "    accuracy                           0.98      3580\n",
      "   macro avg       0.95      0.92      0.93      3580\n",
      "weighted avg       0.98      0.98      0.98      3580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KN = KNeighborsClassifier()\n",
    "KN.fit(X_train,y_train)\n",
    "KN_pred = KN.predict(X_test)\n",
    "print(confusion_matrix(y_test,KN_pred))\n",
    "print(classification_report(y_test,KN_pred))\n",
    "KN_score = precision_score(y_test,KN_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 1s 6ms/step - loss: 0.7634 - accuracy: 0.2395 - val_loss: 0.6681 - val_accuracy: 0.9581\n",
      "Epoch 2/400\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.6506 - accuracy: 0.9398 - val_loss: 0.6362 - val_accuracy: 0.9308\n",
      "Epoch 3/400\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.6229 - accuracy: 0.9406 - val_loss: 0.6086 - val_accuracy: 0.9493\n",
      "Epoch 4/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5951 - accuracy: 0.9533 - val_loss: 0.5804 - val_accuracy: 0.9589\n",
      "Epoch 5/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5666 - accuracy: 0.9610 - val_loss: 0.5513 - val_accuracy: 0.9657\n",
      "Epoch 6/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5376 - accuracy: 0.9665 - val_loss: 0.5222 - val_accuracy: 0.9707\n",
      "Epoch 7/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5092 - accuracy: 0.9683 - val_loss: 0.4942 - val_accuracy: 0.9727\n",
      "Epoch 8/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4824 - accuracy: 0.9697 - val_loss: 0.4682 - val_accuracy: 0.9725\n",
      "Epoch 9/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4579 - accuracy: 0.9713 - val_loss: 0.4445 - val_accuracy: 0.9729\n",
      "Epoch 10/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.9716 - val_loss: 0.4227 - val_accuracy: 0.9737\n",
      "Epoch 11/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4151 - accuracy: 0.9716 - val_loss: 0.4028 - val_accuracy: 0.9737\n",
      "Epoch 12/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3962 - accuracy: 0.9723 - val_loss: 0.3845 - val_accuracy: 0.9743\n",
      "Epoch 13/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3788 - accuracy: 0.9723 - val_loss: 0.3675 - val_accuracy: 0.9745\n",
      "Epoch 14/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.9726 - val_loss: 0.3515 - val_accuracy: 0.9747\n",
      "Epoch 15/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3476 - accuracy: 0.9731 - val_loss: 0.3368 - val_accuracy: 0.9747\n",
      "Epoch 16/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3335 - accuracy: 0.9727 - val_loss: 0.3231 - val_accuracy: 0.9749\n",
      "Epoch 17/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3205 - accuracy: 0.9732 - val_loss: 0.3101 - val_accuracy: 0.9755\n",
      "Epoch 18/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3081 - accuracy: 0.9734 - val_loss: 0.2979 - val_accuracy: 0.9759\n",
      "Epoch 19/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2966 - accuracy: 0.9734 - val_loss: 0.2865 - val_accuracy: 0.9761\n",
      "Epoch 20/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2858 - accuracy: 0.9730 - val_loss: 0.2760 - val_accuracy: 0.9757\n",
      "Epoch 21/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2757 - accuracy: 0.9735 - val_loss: 0.2662 - val_accuracy: 0.9751\n",
      "Epoch 22/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2662 - accuracy: 0.9735 - val_loss: 0.2564 - val_accuracy: 0.9757\n",
      "Epoch 23/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2571 - accuracy: 0.9734 - val_loss: 0.2477 - val_accuracy: 0.9751\n",
      "Epoch 24/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2487 - accuracy: 0.9736 - val_loss: 0.2389 - val_accuracy: 0.9767\n",
      "Epoch 25/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.9742 - val_loss: 0.2316 - val_accuracy: 0.9749\n",
      "Epoch 26/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2332 - accuracy: 0.9737 - val_loss: 0.2236 - val_accuracy: 0.9763\n",
      "Epoch 27/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.9736 - val_loss: 0.2167 - val_accuracy: 0.9755\n",
      "Epoch 28/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.9735 - val_loss: 0.2103 - val_accuracy: 0.9751\n",
      "Epoch 29/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9737 - val_loss: 0.2033 - val_accuracy: 0.9765\n",
      "Epoch 30/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.9739 - val_loss: 0.1977 - val_accuracy: 0.9755\n",
      "Epoch 31/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2011 - accuracy: 0.9739 - val_loss: 0.1922 - val_accuracy: 0.9751\n",
      "Epoch 32/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1957 - accuracy: 0.9743 - val_loss: 0.1863 - val_accuracy: 0.9763\n",
      "Epoch 33/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.9740 - val_loss: 0.1810 - val_accuracy: 0.9765\n",
      "Epoch 34/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.9738 - val_loss: 0.1768 - val_accuracy: 0.9751\n",
      "Epoch 35/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9739 - val_loss: 0.1716 - val_accuracy: 0.9765\n",
      "Epoch 36/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1768 - accuracy: 0.9740 - val_loss: 0.1678 - val_accuracy: 0.9751\n",
      "Epoch 37/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1728 - accuracy: 0.9737 - val_loss: 0.1634 - val_accuracy: 0.9757\n",
      "Epoch 38/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.9744 - val_loss: 0.1594 - val_accuracy: 0.9757\n",
      "Epoch 39/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1649 - accuracy: 0.9740 - val_loss: 0.1555 - val_accuracy: 0.9759\n",
      "Epoch 40/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1613 - accuracy: 0.9745 - val_loss: 0.1517 - val_accuracy: 0.9769\n",
      "Epoch 41/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1577 - accuracy: 0.9745 - val_loss: 0.1485 - val_accuracy: 0.9761\n",
      "Epoch 42/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1545 - accuracy: 0.9746 - val_loss: 0.1466 - val_accuracy: 0.9747\n",
      "Epoch 43/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1514 - accuracy: 0.9744 - val_loss: 0.1422 - val_accuracy: 0.9755\n",
      "Epoch 44/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1482 - accuracy: 0.9745 - val_loss: 0.1396 - val_accuracy: 0.9753\n",
      "Epoch 45/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1455 - accuracy: 0.9743 - val_loss: 0.1363 - val_accuracy: 0.9759\n",
      "Epoch 46/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9744 - val_loss: 0.1336 - val_accuracy: 0.9763\n",
      "Epoch 47/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1402 - accuracy: 0.9747 - val_loss: 0.1311 - val_accuracy: 0.9759\n",
      "Epoch 48/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1377 - accuracy: 0.9744 - val_loss: 0.1286 - val_accuracy: 0.9761\n",
      "Epoch 49/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1351 - accuracy: 0.9747 - val_loss: 0.1262 - val_accuracy: 0.9771\n",
      "Epoch 50/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1330 - accuracy: 0.9747 - val_loss: 0.1239 - val_accuracy: 0.9763\n",
      "Epoch 51/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1307 - accuracy: 0.9744 - val_loss: 0.1224 - val_accuracy: 0.9753\n",
      "Epoch 52/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1288 - accuracy: 0.9745 - val_loss: 0.1199 - val_accuracy: 0.9759\n",
      "Epoch 53/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1269 - accuracy: 0.9747 - val_loss: 0.1178 - val_accuracy: 0.9765\n",
      "Epoch 54/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1249 - accuracy: 0.9746 - val_loss: 0.1159 - val_accuracy: 0.9765\n",
      "Epoch 55/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1231 - accuracy: 0.9745 - val_loss: 0.1141 - val_accuracy: 0.9765\n",
      "Epoch 56/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1215 - accuracy: 0.9750 - val_loss: 0.1127 - val_accuracy: 0.9759\n",
      "Epoch 57/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1198 - accuracy: 0.9746 - val_loss: 0.1110 - val_accuracy: 0.9763\n",
      "Epoch 58/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1182 - accuracy: 0.9749 - val_loss: 0.1094 - val_accuracy: 0.9763\n",
      "Epoch 59/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.9749 - val_loss: 0.1076 - val_accuracy: 0.9765\n",
      "Epoch 60/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1154 - accuracy: 0.9751 - val_loss: 0.1062 - val_accuracy: 0.9775\n",
      "Epoch 61/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1139 - accuracy: 0.9749 - val_loss: 0.1048 - val_accuracy: 0.9771\n",
      "Epoch 62/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1127 - accuracy: 0.9749 - val_loss: 0.1038 - val_accuracy: 0.9759\n",
      "Epoch 63/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9753 - val_loss: 0.1022 - val_accuracy: 0.9767\n",
      "Epoch 64/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1102 - accuracy: 0.9750 - val_loss: 0.1011 - val_accuracy: 0.9765\n",
      "Epoch 65/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1090 - accuracy: 0.9751 - val_loss: 0.0997 - val_accuracy: 0.9773\n",
      "Epoch 66/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1080 - accuracy: 0.9752 - val_loss: 0.0988 - val_accuracy: 0.9761\n",
      "Epoch 67/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.9752 - val_loss: 0.0975 - val_accuracy: 0.9765\n",
      "Epoch 68/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1058 - accuracy: 0.9750 - val_loss: 0.0967 - val_accuracy: 0.9763\n",
      "Epoch 69/400\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.1050 - accuracy: 0.9753 - val_loss: 0.0955 - val_accuracy: 0.9771\n",
      "Epoch 70/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1041 - accuracy: 0.9754 - val_loss: 0.0948 - val_accuracy: 0.9765\n",
      "Epoch 71/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1030 - accuracy: 0.9749 - val_loss: 0.0939 - val_accuracy: 0.9779\n",
      "Epoch 72/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9754 - val_loss: 0.0930 - val_accuracy: 0.9765\n",
      "Epoch 73/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1013 - accuracy: 0.9751 - val_loss: 0.0920 - val_accuracy: 0.9765\n",
      "Epoch 74/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1005 - accuracy: 0.9755 - val_loss: 0.0915 - val_accuracy: 0.9761\n",
      "Epoch 75/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0998 - accuracy: 0.9754 - val_loss: 0.0906 - val_accuracy: 0.9765\n",
      "Epoch 76/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.9755 - val_loss: 0.0901 - val_accuracy: 0.9761\n",
      "Epoch 77/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0982 - accuracy: 0.9755 - val_loss: 0.0897 - val_accuracy: 0.9761\n",
      "Epoch 78/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0976 - accuracy: 0.9752 - val_loss: 0.0884 - val_accuracy: 0.9767\n",
      "Epoch 79/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9754 - val_loss: 0.0877 - val_accuracy: 0.9767\n",
      "Epoch 80/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0961 - accuracy: 0.9751 - val_loss: 0.0874 - val_accuracy: 0.9761\n",
      "Epoch 81/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0957 - accuracy: 0.9753 - val_loss: 0.0865 - val_accuracy: 0.9765\n",
      "Epoch 82/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0952 - accuracy: 0.9755 - val_loss: 0.0859 - val_accuracy: 0.9779\n",
      "Epoch 83/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0944 - accuracy: 0.9758 - val_loss: 0.0859 - val_accuracy: 0.9761\n",
      "Epoch 84/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0940 - accuracy: 0.9760 - val_loss: 0.0853 - val_accuracy: 0.9761\n",
      "Epoch 85/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0936 - accuracy: 0.9758 - val_loss: 0.0847 - val_accuracy: 0.9761\n",
      "Epoch 86/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0929 - accuracy: 0.9757 - val_loss: 0.0839 - val_accuracy: 0.9763\n",
      "Epoch 87/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9757 - val_loss: 0.0835 - val_accuracy: 0.9761\n",
      "Epoch 88/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0919 - accuracy: 0.9757 - val_loss: 0.0826 - val_accuracy: 0.9777\n",
      "Epoch 89/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0915 - accuracy: 0.9753 - val_loss: 0.0823 - val_accuracy: 0.9777\n",
      "Epoch 90/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0911 - accuracy: 0.9755 - val_loss: 0.0818 - val_accuracy: 0.9779\n",
      "Epoch 91/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0905 - accuracy: 0.9758 - val_loss: 0.0813 - val_accuracy: 0.9781\n",
      "Epoch 92/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9761 - val_loss: 0.0812 - val_accuracy: 0.9765\n",
      "Epoch 93/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0898 - accuracy: 0.9755 - val_loss: 0.0809 - val_accuracy: 0.9761\n",
      "Epoch 94/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0893 - accuracy: 0.9759 - val_loss: 0.0801 - val_accuracy: 0.9783\n",
      "Epoch 95/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0888 - accuracy: 0.9759 - val_loss: 0.0800 - val_accuracy: 0.9779\n",
      "Epoch 96/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0884 - accuracy: 0.9761 - val_loss: 0.0793 - val_accuracy: 0.9781\n",
      "Epoch 97/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0881 - accuracy: 0.9758 - val_loss: 0.0790 - val_accuracy: 0.9781\n",
      "Epoch 98/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0877 - accuracy: 0.9759 - val_loss: 0.0786 - val_accuracy: 0.9779\n",
      "Epoch 99/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 0.9761 - val_loss: 0.0788 - val_accuracy: 0.9787\n",
      "Epoch 100/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0872 - accuracy: 0.9757 - val_loss: 0.0779 - val_accuracy: 0.9777\n",
      "Epoch 101/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.9759 - val_loss: 0.0776 - val_accuracy: 0.9777\n",
      "Epoch 102/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.9760 - val_loss: 0.0774 - val_accuracy: 0.9781\n",
      "Epoch 103/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0863 - accuracy: 0.9765 - val_loss: 0.0771 - val_accuracy: 0.9779\n",
      "Epoch 104/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0859 - accuracy: 0.9763 - val_loss: 0.0768 - val_accuracy: 0.9775\n",
      "Epoch 105/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0856 - accuracy: 0.9758 - val_loss: 0.0769 - val_accuracy: 0.9769\n",
      "Epoch 106/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9761 - val_loss: 0.0763 - val_accuracy: 0.9777\n",
      "Epoch 107/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9765 - val_loss: 0.0759 - val_accuracy: 0.9779\n",
      "Epoch 108/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0849 - accuracy: 0.9760 - val_loss: 0.0758 - val_accuracy: 0.9783\n",
      "Epoch 109/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 0.9763 - val_loss: 0.0755 - val_accuracy: 0.9777\n",
      "Epoch 110/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0845 - accuracy: 0.9760 - val_loss: 0.0753 - val_accuracy: 0.9773\n",
      "Epoch 111/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0844 - accuracy: 0.9765 - val_loss: 0.0757 - val_accuracy: 0.9769\n",
      "Epoch 112/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0840 - accuracy: 0.9766 - val_loss: 0.0749 - val_accuracy: 0.9775\n",
      "Epoch 113/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0839 - accuracy: 0.9756 - val_loss: 0.0748 - val_accuracy: 0.9773\n",
      "Epoch 114/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0837 - accuracy: 0.9763 - val_loss: 0.0748 - val_accuracy: 0.9775\n",
      "Epoch 115/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0835 - accuracy: 0.9761 - val_loss: 0.0744 - val_accuracy: 0.9773\n",
      "Epoch 116/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0833 - accuracy: 0.9763 - val_loss: 0.0742 - val_accuracy: 0.9779\n",
      "Epoch 117/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0832 - accuracy: 0.9760 - val_loss: 0.0739 - val_accuracy: 0.9773\n",
      "Epoch 118/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0831 - accuracy: 0.9764 - val_loss: 0.0737 - val_accuracy: 0.9781\n",
      "Epoch 119/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.9766 - val_loss: 0.0739 - val_accuracy: 0.9783\n",
      "Epoch 120/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0828 - accuracy: 0.9761 - val_loss: 0.0735 - val_accuracy: 0.9777\n",
      "Epoch 121/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0825 - accuracy: 0.9763 - val_loss: 0.0734 - val_accuracy: 0.9775\n",
      "Epoch 122/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9766 - val_loss: 0.0737 - val_accuracy: 0.9777\n",
      "Epoch 123/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0824 - accuracy: 0.9765 - val_loss: 0.0731 - val_accuracy: 0.9775\n",
      "Epoch 124/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0822 - accuracy: 0.9765 - val_loss: 0.0731 - val_accuracy: 0.9781\n",
      "Epoch 125/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0822 - accuracy: 0.9766 - val_loss: 0.0730 - val_accuracy: 0.9777\n",
      "Epoch 126/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0820 - accuracy: 0.9768 - val_loss: 0.0728 - val_accuracy: 0.9779\n",
      "Epoch 127/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0820 - accuracy: 0.9765 - val_loss: 0.0727 - val_accuracy: 0.9779\n",
      "Epoch 128/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0819 - accuracy: 0.9766 - val_loss: 0.0728 - val_accuracy: 0.9779\n",
      "Epoch 129/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0817 - accuracy: 0.9763 - val_loss: 0.0726 - val_accuracy: 0.9781\n",
      "Epoch 130/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0816 - accuracy: 0.9765 - val_loss: 0.0723 - val_accuracy: 0.9777\n",
      "Epoch 131/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0815 - accuracy: 0.9763 - val_loss: 0.0722 - val_accuracy: 0.9777\n",
      "Epoch 132/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0814 - accuracy: 0.9767 - val_loss: 0.0721 - val_accuracy: 0.9777\n",
      "Epoch 133/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.9768 - val_loss: 0.0720 - val_accuracy: 0.9779\n",
      "Epoch 134/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0812 - accuracy: 0.9764 - val_loss: 0.0719 - val_accuracy: 0.9781\n",
      "Epoch 135/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.9766 - val_loss: 0.0719 - val_accuracy: 0.9779\n",
      "Epoch 136/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0810 - accuracy: 0.9767 - val_loss: 0.0717 - val_accuracy: 0.9779\n",
      "Epoch 137/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 0.9768 - val_loss: 0.0718 - val_accuracy: 0.9783\n",
      "Epoch 138/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0808 - accuracy: 0.9769 - val_loss: 0.0718 - val_accuracy: 0.9779\n",
      "Epoch 139/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 0.9763 - val_loss: 0.0715 - val_accuracy: 0.9781\n",
      "Epoch 140/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0807 - accuracy: 0.9766 - val_loss: 0.0713 - val_accuracy: 0.9779\n",
      "Epoch 141/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0807 - accuracy: 0.9765 - val_loss: 0.0716 - val_accuracy: 0.9783\n",
      "Epoch 142/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0806 - accuracy: 0.9769 - val_loss: 0.0712 - val_accuracy: 0.9785\n",
      "Epoch 143/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0807 - accuracy: 0.9768 - val_loss: 0.0714 - val_accuracy: 0.9783\n",
      "Epoch 144/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0806 - accuracy: 0.9768 - val_loss: 0.0710 - val_accuracy: 0.9785\n",
      "Epoch 145/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0804 - accuracy: 0.9767 - val_loss: 0.0712 - val_accuracy: 0.9785\n",
      "Epoch 146/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.9764 - val_loss: 0.0712 - val_accuracy: 0.9789\n",
      "Epoch 147/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0803 - accuracy: 0.9764 - val_loss: 0.0712 - val_accuracy: 0.9789\n",
      "Epoch 148/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0801 - accuracy: 0.9771 - val_loss: 0.0709 - val_accuracy: 0.9785\n",
      "Epoch 149/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0802 - accuracy: 0.9766 - val_loss: 0.0710 - val_accuracy: 0.9783\n",
      "Epoch 150/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0799 - accuracy: 0.9767 - val_loss: 0.0708 - val_accuracy: 0.9787\n",
      "Epoch 151/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0800 - accuracy: 0.9770 - val_loss: 0.0712 - val_accuracy: 0.9785\n",
      "Epoch 152/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9768 - val_loss: 0.0708 - val_accuracy: 0.9783\n",
      "Epoch 153/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0798 - accuracy: 0.9770 - val_loss: 0.0707 - val_accuracy: 0.9783\n",
      "Epoch 154/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9767 - val_loss: 0.0707 - val_accuracy: 0.9783\n",
      "Epoch 155/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9768 - val_loss: 0.0705 - val_accuracy: 0.9787\n",
      "Epoch 156/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9769 - val_loss: 0.0704 - val_accuracy: 0.9787\n",
      "Epoch 157/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0795 - accuracy: 0.9767 - val_loss: 0.0705 - val_accuracy: 0.9785\n",
      "Epoch 158/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0798 - accuracy: 0.9773 - val_loss: 0.0715 - val_accuracy: 0.9781\n",
      "Epoch 159/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9769 - val_loss: 0.0703 - val_accuracy: 0.9785\n",
      "Epoch 160/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0794 - accuracy: 0.9766 - val_loss: 0.0704 - val_accuracy: 0.9783\n",
      "Epoch 161/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9768 - val_loss: 0.0705 - val_accuracy: 0.9787\n",
      "Epoch 162/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0793 - accuracy: 0.9767 - val_loss: 0.0706 - val_accuracy: 0.9787\n",
      "Epoch 163/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0792 - accuracy: 0.9768 - val_loss: 0.0700 - val_accuracy: 0.9791\n",
      "Epoch 164/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0791 - accuracy: 0.9768 - val_loss: 0.0700 - val_accuracy: 0.9785\n",
      "Epoch 165/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0792 - accuracy: 0.9767 - val_loss: 0.0698 - val_accuracy: 0.9789\n",
      "Epoch 166/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9769 - val_loss: 0.0697 - val_accuracy: 0.9787\n",
      "Epoch 167/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0789 - accuracy: 0.9770 - val_loss: 0.0700 - val_accuracy: 0.9789\n",
      "Epoch 168/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0790 - accuracy: 0.9768 - val_loss: 0.0698 - val_accuracy: 0.9787\n",
      "Epoch 169/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0789 - accuracy: 0.9771 - val_loss: 0.0698 - val_accuracy: 0.9787\n",
      "Epoch 170/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0788 - accuracy: 0.9771 - val_loss: 0.0697 - val_accuracy: 0.9789\n",
      "Epoch 171/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9770 - val_loss: 0.0698 - val_accuracy: 0.9787\n",
      "Epoch 172/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9769 - val_loss: 0.0701 - val_accuracy: 0.9789\n",
      "Epoch 173/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0786 - accuracy: 0.9768 - val_loss: 0.0697 - val_accuracy: 0.9789\n",
      "Epoch 174/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.9768 - val_loss: 0.0695 - val_accuracy: 0.9792\n",
      "Epoch 175/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9770 - val_loss: 0.0695 - val_accuracy: 0.9789\n",
      "Epoch 176/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0785 - accuracy: 0.9768 - val_loss: 0.0695 - val_accuracy: 0.9791\n",
      "Epoch 177/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0784 - accuracy: 0.9770 - val_loss: 0.0695 - val_accuracy: 0.9791\n",
      "Epoch 178/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0784 - accuracy: 0.9766 - val_loss: 0.0693 - val_accuracy: 0.9792\n",
      "Epoch 179/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0784 - accuracy: 0.9768 - val_loss: 0.0692 - val_accuracy: 0.9792\n",
      "Epoch 180/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0783 - accuracy: 0.9770 - val_loss: 0.0694 - val_accuracy: 0.9791\n",
      "Epoch 181/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0783 - accuracy: 0.9770 - val_loss: 0.0692 - val_accuracy: 0.9789\n",
      "Epoch 182/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0781 - accuracy: 0.9773 - val_loss: 0.0695 - val_accuracy: 0.9792\n",
      "Epoch 183/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9767 - val_loss: 0.0690 - val_accuracy: 0.9791\n",
      "Epoch 184/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0780 - accuracy: 0.9770 - val_loss: 0.0694 - val_accuracy: 0.9792\n",
      "Epoch 185/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9771 - val_loss: 0.0694 - val_accuracy: 0.9792\n",
      "Epoch 186/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0782 - accuracy: 0.9772 - val_loss: 0.0690 - val_accuracy: 0.9792\n",
      "Epoch 187/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0783 - accuracy: 0.9769 - val_loss: 0.0689 - val_accuracy: 0.9791\n",
      "Epoch 188/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0780 - accuracy: 0.9771 - val_loss: 0.0686 - val_accuracy: 0.9791\n",
      "Epoch 189/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0779 - accuracy: 0.9770 - val_loss: 0.0689 - val_accuracy: 0.9791\n",
      "Epoch 190/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0779 - accuracy: 0.9766 - val_loss: 0.0687 - val_accuracy: 0.9791\n",
      "Epoch 191/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0779 - accuracy: 0.9774 - val_loss: 0.0688 - val_accuracy: 0.9794\n",
      "Epoch 192/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9768 - val_loss: 0.0687 - val_accuracy: 0.9792\n",
      "Epoch 193/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0778 - accuracy: 0.9772 - val_loss: 0.0689 - val_accuracy: 0.9789\n",
      "Epoch 194/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0777 - accuracy: 0.9770 - val_loss: 0.0685 - val_accuracy: 0.9792\n",
      "Epoch 195/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0776 - accuracy: 0.9770 - val_loss: 0.0685 - val_accuracy: 0.9791\n",
      "Epoch 196/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0776 - accuracy: 0.9768 - val_loss: 0.0686 - val_accuracy: 0.9789\n",
      "Epoch 197/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.9770 - val_loss: 0.0686 - val_accuracy: 0.9794\n",
      "Epoch 198/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0776 - accuracy: 0.9768 - val_loss: 0.0687 - val_accuracy: 0.9792\n",
      "Epoch 199/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9769 - val_loss: 0.0683 - val_accuracy: 0.9792\n",
      "Epoch 200/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 0.9770 - val_loss: 0.0685 - val_accuracy: 0.9792\n",
      "Epoch 201/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9770 - val_loss: 0.0683 - val_accuracy: 0.9794\n",
      "Epoch 202/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0773 - accuracy: 0.9768 - val_loss: 0.0686 - val_accuracy: 0.9787\n",
      "Epoch 203/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0778 - accuracy: 0.9774 - val_loss: 0.0682 - val_accuracy: 0.9792\n",
      "Epoch 204/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0773 - accuracy: 0.9771 - val_loss: 0.0683 - val_accuracy: 0.9791\n",
      "Epoch 205/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 0.9769 - val_loss: 0.0684 - val_accuracy: 0.9787\n",
      "Epoch 206/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9771 - val_loss: 0.0680 - val_accuracy: 0.9792\n",
      "Epoch 207/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9771 - val_loss: 0.0683 - val_accuracy: 0.9796\n",
      "Epoch 208/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0770 - accuracy: 0.9775 - val_loss: 0.0682 - val_accuracy: 0.9792\n",
      "Epoch 209/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9770 - val_loss: 0.0682 - val_accuracy: 0.9792\n",
      "Epoch 210/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9772 - val_loss: 0.0682 - val_accuracy: 0.9792\n",
      "Epoch 211/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9776 - val_loss: 0.0678 - val_accuracy: 0.9794\n",
      "Epoch 212/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0772 - accuracy: 0.9773 - val_loss: 0.0678 - val_accuracy: 0.9794\n",
      "Epoch 213/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0770 - accuracy: 0.9772 - val_loss: 0.0681 - val_accuracy: 0.9792\n",
      "Epoch 214/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0772 - accuracy: 0.9776 - val_loss: 0.0679 - val_accuracy: 0.9792\n",
      "Epoch 215/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.9772 - val_loss: 0.0677 - val_accuracy: 0.9792\n",
      "Epoch 216/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.9770 - val_loss: 0.0677 - val_accuracy: 0.9794\n",
      "Epoch 217/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.9775 - val_loss: 0.0678 - val_accuracy: 0.9792\n",
      "Epoch 218/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.9781 - val_loss: 0.0682 - val_accuracy: 0.9794\n",
      "Epoch 219/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0768 - accuracy: 0.9770 - val_loss: 0.0678 - val_accuracy: 0.9796\n",
      "Epoch 220/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.9778 - val_loss: 0.0678 - val_accuracy: 0.9794\n",
      "Epoch 221/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.9775 - val_loss: 0.0682 - val_accuracy: 0.9794\n",
      "Epoch 222/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0769 - accuracy: 0.9774 - val_loss: 0.0686 - val_accuracy: 0.9792\n",
      "Epoch 223/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9772 - val_loss: 0.0676 - val_accuracy: 0.9791\n",
      "Epoch 224/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.9771 - val_loss: 0.0677 - val_accuracy: 0.9794\n",
      "Epoch 225/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9771 - val_loss: 0.0679 - val_accuracy: 0.9792\n",
      "Epoch 226/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9779 - val_loss: 0.0675 - val_accuracy: 0.9792\n",
      "Epoch 227/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9775 - val_loss: 0.0675 - val_accuracy: 0.9794\n",
      "Epoch 228/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9776 - val_loss: 0.0673 - val_accuracy: 0.9796\n",
      "Epoch 229/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0765 - accuracy: 0.9770 - val_loss: 0.0673 - val_accuracy: 0.9796\n",
      "Epoch 230/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9772 - val_loss: 0.0679 - val_accuracy: 0.9792\n",
      "Epoch 231/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0766 - accuracy: 0.9774 - val_loss: 0.0672 - val_accuracy: 0.9792\n",
      "Epoch 232/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0764 - accuracy: 0.9774 - val_loss: 0.0674 - val_accuracy: 0.9794\n",
      "Epoch 233/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9776 - val_loss: 0.0672 - val_accuracy: 0.9794\n",
      "Epoch 234/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0764 - accuracy: 0.9774 - val_loss: 0.0673 - val_accuracy: 0.9794\n",
      "Epoch 235/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0761 - accuracy: 0.9775 - val_loss: 0.0672 - val_accuracy: 0.9794\n",
      "Epoch 236/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0765 - accuracy: 0.9771 - val_loss: 0.0673 - val_accuracy: 0.9792\n",
      "Epoch 237/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0763 - accuracy: 0.9782 - val_loss: 0.0670 - val_accuracy: 0.9794\n",
      "Epoch 238/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0761 - accuracy: 0.9774 - val_loss: 0.0670 - val_accuracy: 0.9794\n",
      "Epoch 239/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9776 - val_loss: 0.0672 - val_accuracy: 0.9796\n",
      "Epoch 240/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0766 - accuracy: 0.9774 - val_loss: 0.0676 - val_accuracy: 0.9796\n",
      "Epoch 241/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9779 - val_loss: 0.0670 - val_accuracy: 0.9794\n",
      "Epoch 242/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0763 - accuracy: 0.9773 - val_loss: 0.0670 - val_accuracy: 0.9796\n",
      "Epoch 243/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0760 - accuracy: 0.9774 - val_loss: 0.0669 - val_accuracy: 0.9792\n",
      "Epoch 244/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.9774 - val_loss: 0.0670 - val_accuracy: 0.9794\n",
      "Epoch 245/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0759 - accuracy: 0.9778 - val_loss: 0.0669 - val_accuracy: 0.9792\n",
      "Epoch 246/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0761 - accuracy: 0.9779 - val_loss: 0.0669 - val_accuracy: 0.9798\n",
      "Epoch 247/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0759 - accuracy: 0.9776 - val_loss: 0.0668 - val_accuracy: 0.9794\n",
      "Epoch 248/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0757 - accuracy: 0.9779 - val_loss: 0.0676 - val_accuracy: 0.9792\n",
      "Epoch 249/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0758 - accuracy: 0.9781 - val_loss: 0.0668 - val_accuracy: 0.9794\n",
      "Epoch 250/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0760 - accuracy: 0.9775 - val_loss: 0.0669 - val_accuracy: 0.9796\n",
      "Epoch 251/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.9778 - val_loss: 0.0667 - val_accuracy: 0.9796\n",
      "Epoch 252/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9776 - val_loss: 0.0669 - val_accuracy: 0.9792\n",
      "Epoch 253/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0757 - accuracy: 0.9775 - val_loss: 0.0665 - val_accuracy: 0.9796\n",
      "Epoch 254/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0758 - accuracy: 0.9778 - val_loss: 0.0667 - val_accuracy: 0.9796\n",
      "Epoch 255/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9780 - val_loss: 0.0668 - val_accuracy: 0.9798\n",
      "Epoch 256/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.9779 - val_loss: 0.0664 - val_accuracy: 0.9796\n",
      "Epoch 257/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.9778 - val_loss: 0.0664 - val_accuracy: 0.9794\n",
      "Epoch 258/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0758 - accuracy: 0.9781 - val_loss: 0.0664 - val_accuracy: 0.9798\n",
      "Epoch 259/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0757 - accuracy: 0.9781 - val_loss: 0.0664 - val_accuracy: 0.9796\n",
      "Epoch 260/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0757 - accuracy: 0.9780 - val_loss: 0.0664 - val_accuracy: 0.9796\n",
      "Epoch 261/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9778 - val_loss: 0.0663 - val_accuracy: 0.9796\n",
      "Epoch 262/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0758 - accuracy: 0.9782 - val_loss: 0.0663 - val_accuracy: 0.9798\n",
      "Epoch 263/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9776 - val_loss: 0.0662 - val_accuracy: 0.9798\n",
      "Epoch 264/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.9782 - val_loss: 0.0671 - val_accuracy: 0.9794\n",
      "Epoch 265/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9780 - val_loss: 0.0666 - val_accuracy: 0.9794\n",
      "Epoch 266/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9783 - val_loss: 0.0663 - val_accuracy: 0.9792\n",
      "Epoch 267/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0753 - accuracy: 0.9785 - val_loss: 0.0665 - val_accuracy: 0.9796\n",
      "Epoch 268/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0753 - accuracy: 0.9784 - val_loss: 0.0659 - val_accuracy: 0.9792\n",
      "Epoch 269/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0752 - accuracy: 0.9779 - val_loss: 0.0662 - val_accuracy: 0.9800\n",
      "Epoch 270/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0752 - accuracy: 0.9778 - val_loss: 0.0661 - val_accuracy: 0.9794\n",
      "Epoch 271/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9782 - val_loss: 0.0660 - val_accuracy: 0.9800\n",
      "Epoch 272/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9779 - val_loss: 0.0662 - val_accuracy: 0.9796\n",
      "Epoch 273/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0753 - accuracy: 0.9776 - val_loss: 0.0661 - val_accuracy: 0.9800\n",
      "Epoch 274/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9780 - val_loss: 0.0658 - val_accuracy: 0.9794\n",
      "Epoch 275/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9782 - val_loss: 0.0660 - val_accuracy: 0.9798\n",
      "Epoch 276/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0752 - accuracy: 0.9781 - val_loss: 0.0658 - val_accuracy: 0.9798\n",
      "Epoch 277/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9778 - val_loss: 0.0660 - val_accuracy: 0.9798\n",
      "Epoch 278/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.9786 - val_loss: 0.0656 - val_accuracy: 0.9794\n",
      "Epoch 279/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9781 - val_loss: 0.0661 - val_accuracy: 0.9800\n",
      "Epoch 280/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9778 - val_loss: 0.0662 - val_accuracy: 0.9794\n",
      "Epoch 281/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9782 - val_loss: 0.0657 - val_accuracy: 0.9796\n",
      "Epoch 282/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0749 - accuracy: 0.9779 - val_loss: 0.0664 - val_accuracy: 0.9804\n",
      "Epoch 283/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9786 - val_loss: 0.0661 - val_accuracy: 0.9802\n",
      "Epoch 284/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0747 - accuracy: 0.9783 - val_loss: 0.0657 - val_accuracy: 0.9798\n",
      "Epoch 285/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9779 - val_loss: 0.0661 - val_accuracy: 0.9796\n",
      "Epoch 286/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0747 - accuracy: 0.9781 - val_loss: 0.0659 - val_accuracy: 0.9796\n",
      "Epoch 287/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9781 - val_loss: 0.0661 - val_accuracy: 0.9796\n",
      "Epoch 288/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0748 - accuracy: 0.9788 - val_loss: 0.0659 - val_accuracy: 0.9794\n",
      "Epoch 289/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0747 - accuracy: 0.9781 - val_loss: 0.0659 - val_accuracy: 0.9800\n",
      "Epoch 290/400\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0749 - accuracy: 0.9784 - val_loss: 0.0659 - val_accuracy: 0.9806\n",
      "Epoch 291/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0746 - accuracy: 0.9786 - val_loss: 0.0656 - val_accuracy: 0.9800\n",
      "Epoch 292/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0745 - accuracy: 0.9781 - val_loss: 0.0657 - val_accuracy: 0.9800\n",
      "Epoch 293/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9785 - val_loss: 0.0659 - val_accuracy: 0.9802\n",
      "Epoch 294/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0745 - accuracy: 0.9784 - val_loss: 0.0659 - val_accuracy: 0.9798\n",
      "Epoch 295/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.9784 - val_loss: 0.0658 - val_accuracy: 0.9808\n",
      "Epoch 296/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0747 - accuracy: 0.9788 - val_loss: 0.0655 - val_accuracy: 0.9804\n",
      "Epoch 297/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0746 - accuracy: 0.9778 - val_loss: 0.0654 - val_accuracy: 0.9796\n",
      "Epoch 298/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0742 - accuracy: 0.9785 - val_loss: 0.0655 - val_accuracy: 0.9802\n",
      "Epoch 299/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0743 - accuracy: 0.9786 - val_loss: 0.0657 - val_accuracy: 0.9798\n",
      "Epoch 300/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0745 - accuracy: 0.9784 - val_loss: 0.0655 - val_accuracy: 0.9800\n",
      "Epoch 301/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9786 - val_loss: 0.0655 - val_accuracy: 0.9800\n",
      "Epoch 302/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9789 - val_loss: 0.0667 - val_accuracy: 0.9791\n",
      "Epoch 303/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0746 - accuracy: 0.9783 - val_loss: 0.0656 - val_accuracy: 0.9800\n",
      "Epoch 304/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9784 - val_loss: 0.0652 - val_accuracy: 0.9796\n",
      "Epoch 305/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0742 - accuracy: 0.9784 - val_loss: 0.0657 - val_accuracy: 0.9808\n",
      "Epoch 306/400\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.9786 - val_loss: 0.0653 - val_accuracy: 0.9802\n",
      "Epoch 307/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9783 - val_loss: 0.0655 - val_accuracy: 0.9802\n",
      "Epoch 308/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9785 - val_loss: 0.0656 - val_accuracy: 0.9810\n",
      "Epoch 309/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9787 - val_loss: 0.0651 - val_accuracy: 0.9796\n",
      "Epoch 310/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9785 - val_loss: 0.0653 - val_accuracy: 0.9802\n",
      "Epoch 311/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.9784 - val_loss: 0.0650 - val_accuracy: 0.9802\n",
      "Epoch 312/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9787 - val_loss: 0.0655 - val_accuracy: 0.9804\n",
      "Epoch 313/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0743 - accuracy: 0.9787 - val_loss: 0.0658 - val_accuracy: 0.9808\n",
      "Epoch 314/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9786 - val_loss: 0.0651 - val_accuracy: 0.9802\n",
      "Epoch 315/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0740 - accuracy: 0.9790 - val_loss: 0.0651 - val_accuracy: 0.9802\n",
      "Epoch 316/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.9786 - val_loss: 0.0651 - val_accuracy: 0.9800\n",
      "Epoch 317/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0740 - accuracy: 0.9784 - val_loss: 0.0650 - val_accuracy: 0.9802\n",
      "Epoch 318/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9789 - val_loss: 0.0650 - val_accuracy: 0.9798\n",
      "Epoch 319/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9783 - val_loss: 0.0651 - val_accuracy: 0.9808\n",
      "Epoch 320/400\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9786 - val_loss: 0.0661 - val_accuracy: 0.9808\n",
      "Epoch 321/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9786 - val_loss: 0.0650 - val_accuracy: 0.9804\n",
      "Epoch 322/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9790 - val_loss: 0.0652 - val_accuracy: 0.9804\n",
      "Epoch 323/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9783 - val_loss: 0.0650 - val_accuracy: 0.9800\n",
      "Epoch 324/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9789 - val_loss: 0.0654 - val_accuracy: 0.9806\n",
      "Epoch 325/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9790 - val_loss: 0.0654 - val_accuracy: 0.9808\n",
      "Epoch 326/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0737 - accuracy: 0.9788 - val_loss: 0.0650 - val_accuracy: 0.9802\n",
      "Epoch 327/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9787 - val_loss: 0.0653 - val_accuracy: 0.9802\n",
      "Epoch 328/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0738 - accuracy: 0.9790 - val_loss: 0.0649 - val_accuracy: 0.9802\n",
      "Epoch 329/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0734 - accuracy: 0.9788 - val_loss: 0.0656 - val_accuracy: 0.9808\n",
      "Epoch 330/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9792 - val_loss: 0.0654 - val_accuracy: 0.9806\n",
      "Epoch 331/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 0.9787 - val_loss: 0.0648 - val_accuracy: 0.9802\n",
      "Epoch 332/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0734 - accuracy: 0.9789 - val_loss: 0.0646 - val_accuracy: 0.9802\n",
      "Epoch 333/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 0.9788 - val_loss: 0.0648 - val_accuracy: 0.9802\n",
      "Epoch 334/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9788 - val_loss: 0.0650 - val_accuracy: 0.9804\n",
      "Epoch 335/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.9790 - val_loss: 0.0649 - val_accuracy: 0.9802\n",
      "Epoch 336/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0734 - accuracy: 0.9789 - val_loss: 0.0649 - val_accuracy: 0.9802\n",
      "Epoch 337/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 0.9790 - val_loss: 0.0646 - val_accuracy: 0.9802\n",
      "Epoch 338/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9787 - val_loss: 0.0645 - val_accuracy: 0.9800\n",
      "Epoch 339/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.9792 - val_loss: 0.0645 - val_accuracy: 0.9802\n",
      "Epoch 340/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 0.9784 - val_loss: 0.0647 - val_accuracy: 0.9804\n",
      "Epoch 341/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 0.9794 - val_loss: 0.0648 - val_accuracy: 0.9802\n",
      "Epoch 342/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 0.9789 - val_loss: 0.0644 - val_accuracy: 0.9802\n",
      "Epoch 343/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0731 - accuracy: 0.9787 - val_loss: 0.0649 - val_accuracy: 0.9806\n",
      "Epoch 344/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9794 - val_loss: 0.0644 - val_accuracy: 0.9806\n",
      "Epoch 345/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0729 - accuracy: 0.9789 - val_loss: 0.0646 - val_accuracy: 0.9804\n",
      "Epoch 346/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.9789 - val_loss: 0.0648 - val_accuracy: 0.9804\n",
      "Epoch 347/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0730 - accuracy: 0.9795 - val_loss: 0.0647 - val_accuracy: 0.9804\n",
      "Epoch 348/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9787 - val_loss: 0.0647 - val_accuracy: 0.9806\n",
      "Epoch 349/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0731 - accuracy: 0.9790 - val_loss: 0.0645 - val_accuracy: 0.9802\n",
      "Epoch 350/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9789 - val_loss: 0.0649 - val_accuracy: 0.9802\n",
      "Epoch 351/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.9792 - val_loss: 0.0650 - val_accuracy: 0.9806\n",
      "Epoch 352/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0730 - accuracy: 0.9793 - val_loss: 0.0654 - val_accuracy: 0.9806\n",
      "Epoch 353/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0731 - accuracy: 0.9790 - val_loss: 0.0648 - val_accuracy: 0.9804\n",
      "Epoch 354/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9789 - val_loss: 0.0650 - val_accuracy: 0.9804\n",
      "Epoch 355/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0728 - accuracy: 0.9789 - val_loss: 0.0645 - val_accuracy: 0.9802\n",
      "Epoch 356/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0731 - accuracy: 0.9788 - val_loss: 0.0646 - val_accuracy: 0.9804\n",
      "Epoch 357/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.9793 - val_loss: 0.0646 - val_accuracy: 0.9802\n",
      "Epoch 358/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.9792 - val_loss: 0.0645 - val_accuracy: 0.9802\n",
      "Epoch 359/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.9786 - val_loss: 0.0660 - val_accuracy: 0.9812\n",
      "Epoch 360/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0729 - accuracy: 0.9788 - val_loss: 0.0649 - val_accuracy: 0.9800\n",
      "Epoch 361/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0731 - accuracy: 0.9792 - val_loss: 0.0643 - val_accuracy: 0.9806\n",
      "Epoch 362/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9792 - val_loss: 0.0645 - val_accuracy: 0.9804\n",
      "Epoch 363/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9790 - val_loss: 0.0645 - val_accuracy: 0.9806\n",
      "Epoch 364/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.9787 - val_loss: 0.0644 - val_accuracy: 0.9810\n",
      "Epoch 365/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.9794 - val_loss: 0.0643 - val_accuracy: 0.9802\n",
      "Epoch 366/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.9793 - val_loss: 0.0644 - val_accuracy: 0.9804\n",
      "Epoch 367/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0729 - accuracy: 0.9789 - val_loss: 0.0647 - val_accuracy: 0.9804\n",
      "Epoch 368/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0724 - accuracy: 0.9792 - val_loss: 0.0647 - val_accuracy: 0.9808\n",
      "Epoch 369/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9792 - val_loss: 0.0645 - val_accuracy: 0.9804\n",
      "Epoch 370/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0724 - accuracy: 0.9790 - val_loss: 0.0643 - val_accuracy: 0.9804\n",
      "Epoch 371/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0725 - accuracy: 0.9793 - val_loss: 0.0643 - val_accuracy: 0.9806\n",
      "Epoch 372/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.9793 - val_loss: 0.0644 - val_accuracy: 0.9804\n",
      "Epoch 373/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0725 - accuracy: 0.9785 - val_loss: 0.0648 - val_accuracy: 0.9808\n",
      "Epoch 374/400\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.9795 - val_loss: 0.0642 - val_accuracy: 0.9806\n",
      "Epoch 375/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9794 - val_loss: 0.0642 - val_accuracy: 0.9806\n",
      "Epoch 376/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.9795 - val_loss: 0.0641 - val_accuracy: 0.9808\n",
      "Epoch 377/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0724 - accuracy: 0.9792 - val_loss: 0.0641 - val_accuracy: 0.9808\n",
      "Epoch 378/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 0.9788 - val_loss: 0.0651 - val_accuracy: 0.9804\n",
      "Epoch 379/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0720 - accuracy: 0.9795 - val_loss: 0.0642 - val_accuracy: 0.9808\n",
      "Epoch 380/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.9790 - val_loss: 0.0646 - val_accuracy: 0.9802\n",
      "Epoch 381/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9792 - val_loss: 0.0650 - val_accuracy: 0.9806\n",
      "Epoch 382/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 0.9792 - val_loss: 0.0650 - val_accuracy: 0.9810\n",
      "Epoch 383/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9788 - val_loss: 0.0644 - val_accuracy: 0.9802\n",
      "Epoch 384/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9794 - val_loss: 0.0643 - val_accuracy: 0.9804\n",
      "Epoch 385/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0723 - accuracy: 0.9792 - val_loss: 0.0642 - val_accuracy: 0.9808\n",
      "Epoch 386/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 0.9793 - val_loss: 0.0643 - val_accuracy: 0.9804\n",
      "Epoch 387/400\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0722 - accuracy: 0.9794 - val_loss: 0.0644 - val_accuracy: 0.9812\n",
      "Epoch 388/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9789 - val_loss: 0.0642 - val_accuracy: 0.9804\n",
      "Epoch 389/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9793 - val_loss: 0.0643 - val_accuracy: 0.9804\n",
      "Epoch 390/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.9794 - val_loss: 0.0644 - val_accuracy: 0.9806\n",
      "Epoch 391/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0720 - accuracy: 0.9795 - val_loss: 0.0642 - val_accuracy: 0.9806\n",
      "Epoch 392/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0720 - accuracy: 0.9793 - val_loss: 0.0646 - val_accuracy: 0.9802\n",
      "Epoch 393/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 0.9796 - val_loss: 0.0647 - val_accuracy: 0.9810\n",
      "Epoch 394/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.9794 - val_loss: 0.0643 - val_accuracy: 0.9806\n",
      "Epoch 395/400\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.9798 - val_loss: 0.0649 - val_accuracy: 0.9796\n",
      "Epoch 396/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0724 - accuracy: 0.9794 - val_loss: 0.0642 - val_accuracy: 0.9804\n",
      "Epoch 397/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0720 - accuracy: 0.9793 - val_loss: 0.0644 - val_accuracy: 0.9802\n",
      "Epoch 398/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 0.9790 - val_loss: 0.0645 - val_accuracy: 0.9810\n",
      "Epoch 399/400\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0720 - accuracy: 0.9794 - val_loss: 0.0651 - val_accuracy: 0.9808\n",
      "Epoch 400/400\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.9795 - val_loss: 0.0642 - val_accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout  \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "epochs = 400\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.01, patience=8, verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_without_early_heart=model.fit(X_train, y_train, validation_split=0.35, epochs=epochs, batch_size=128)\n",
    "\n",
    "SEQ_score = model_without_early_heart.history.get(\"val_accuracy\")[epochs-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are hired by IBM Watson as a Data Scientist, Congratulations!\n",
    "#Your first project;  you have to classify heart disesae  based on their medical measurements,  0 healthy, 1-heart disease\n",
    "#your job is to use ML techniques as SVM, Decision_tree, Random_forest and Kneighbours (bonus if you use a Neural Network)\n",
    "#which ML model gives the best?\n",
    "#Because you are working in IBM Watson, they have very clean dataset(there is no preprocessing!)\n",
    "#to learn more about the Heart Disease Dataset :\n",
    "#  https://archive.ics.uci.edu/ml/datasets/Heart+Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>152</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "293   67    1   2       152   212    0        0      150      0      0.8   \n",
       "294   44    1   0       120   169    0        1      144      1      2.8   \n",
       "295   63    1   0       140   187    0        0      144      1      4.0   \n",
       "296   63    0   0       124   197    0        1      136      1      0.0   \n",
       "297   59    1   0       164   176    1        0       90      0      1.0   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "293      1   0     3       0  \n",
       "294      0   0     1       0  \n",
       "295      2   2     3       0  \n",
       "296      1   0     2       0  \n",
       "297      1   2     1       0  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HeartsDataset = pd.read_csv('HeartDiseaseDataset.csv')\n",
    "HeartsDataset.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HeartsDataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = HeartsDataset.drop('target', axis=1)\n",
    "y = HeartsDataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y, test_size = 0.2, shuffle = True)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.690476</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.210046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.259542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.357143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.098174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.557252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.267176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306452</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.880952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.401826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.610687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.547619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.180365</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.862595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.450382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.386792</td>\n",
       "      <td>0.175799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.465649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.412214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.496183</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.386792</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  sex        cp  trestbps      chol  fbs  restecg   thalach  \\\n",
       "276  0.690476  1.0  0.000000  0.490566  0.210046  0.0      0.5  0.259542   \n",
       "294  0.357143  1.0  0.000000  0.245283  0.098174  0.0      0.5  0.557252   \n",
       "252  0.785714  0.0  0.000000  0.415094  0.383562  1.0      0.5  0.267176   \n",
       "51   0.880952  1.0  0.000000  0.245283  0.401826  0.0      0.0  0.610687   \n",
       "78   0.547619  1.0  0.333333  0.320755  0.180365  1.0      0.5  0.862595   \n",
       "..        ...  ...       ...       ...       ...  ...      ...       ...   \n",
       "227  0.142857  1.0  0.000000  0.245283  0.164384  0.0      0.5  0.450382   \n",
       "63   0.285714  1.0  0.333333  0.386792  0.175799  0.0      0.5  0.465649   \n",
       "225  0.976190  1.0  0.000000  0.481132  0.109589  0.0      0.5  0.412214   \n",
       "296  0.809524  0.0  0.000000  0.283019  0.162100  0.0      0.5  0.496183   \n",
       "218  0.857143  1.0  0.000000  0.386792  0.292237  0.0      0.0  0.427481   \n",
       "\n",
       "     exang   oldpeak  slope    ca      thal  \n",
       "276    0.0  0.322581    0.5  0.25  1.000000  \n",
       "294    1.0  0.451613    0.0  0.00  0.333333  \n",
       "252    0.0  0.306452    0.5  0.75  0.666667  \n",
       "51     0.0  0.064516    0.5  0.00  0.666667  \n",
       "78     0.0  0.000000    1.0  0.00  0.666667  \n",
       "..     ...       ...    ...   ...       ...  \n",
       "227    1.0  0.258065    0.5  0.00  1.000000  \n",
       "63     0.0  0.000000    0.5  0.00  0.333333  \n",
       "225    1.0  0.419355    0.0  0.00  1.000000  \n",
       "296    1.0  0.000000    0.5  0.00  0.666667  \n",
       "218    0.0  0.451613    0.5  0.25  1.000000  \n",
       "\n",
       "[242 rows x 13 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.DataFrame(data=scaler.transform(X_train),columns = X_train.columns,index=X_train.index)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.595238</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.257991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.419847</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.380952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.415525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.216895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.709924</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.326484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.381679</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.523810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.549618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.452381</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.549618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.321918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.748092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.904762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.235160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442748</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  sex        cp  trestbps      chol  fbs  restecg   thalach  \\\n",
       "224  0.595238  1.0  0.000000  0.150943  0.257991  0.0      0.5  0.419847   \n",
       "81   0.380952  1.0  0.333333  0.320755  0.415525  0.0      0.0  0.755725   \n",
       "77   0.714286  1.0  0.333333  0.433962  0.216895  0.0      0.5  0.709924   \n",
       "129  1.071429  0.0  0.333333  0.245283  0.326484  0.0      0.0  0.381679   \n",
       "66   0.523810  1.0  0.666667  0.056604  0.219178  0.0      0.5  0.549618   \n",
       "..        ...  ...       ...       ...       ...  ...      ...       ...   \n",
       "56   0.452381  1.0  0.000000  0.264151  0.219178  0.0      0.0  0.877863   \n",
       "126  0.428571  1.0  0.000000  0.169811  0.178082  0.0      0.5  0.549618   \n",
       "123  0.595238  0.0  0.666667  0.132075  0.321918  0.0      0.0  0.732824   \n",
       "182  0.761905  0.0  0.000000  0.339623  0.465753  0.0      0.0  0.748092   \n",
       "166  0.904762  1.0  0.000000  0.245283  0.235160  0.0      0.0  0.442748   \n",
       "\n",
       "     exang   oldpeak  slope    ca      thal  \n",
       "224    1.0  0.451613    0.5  0.25  1.000000  \n",
       "81     0.0  0.000000    1.0  0.00  0.666667  \n",
       "77     1.0  0.000000    1.0  0.00  0.666667  \n",
       "129    1.0  0.032258    1.0  0.25  0.666667  \n",
       "66     1.0  0.193548    0.5  0.00  0.666667  \n",
       "..     ...       ...    ...   ...       ...  \n",
       "56     0.0  0.000000    1.0  0.00  0.666667  \n",
       "126    0.0  0.016129    1.0  0.00  0.666667  \n",
       "123    0.0  0.000000    1.0  0.00  0.666667  \n",
       "182    0.0  0.000000    1.0  0.00  0.666667  \n",
       "166    1.0  0.419355    0.5  0.50  1.000000  \n",
       "\n",
       "[61 rows x 13 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.DataFrame(data=scaler.transform(X_test),columns = X_test.columns,index=X_test.index)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "SVC_score_heart = precision_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22  7]\n",
      " [ 7 25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76        29\n",
      "           1       0.78      0.78      0.78        32\n",
      "\n",
      "    accuracy                           0.77        61\n",
      "   macro avg       0.77      0.77      0.77        61\n",
      "weighted avg       0.77      0.77      0.77        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  9]\n",
      " [ 9 23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69        29\n",
      "           1       0.72      0.72      0.72        32\n",
      "\n",
      "    accuracy                           0.70        61\n",
      "   macro avg       0.70      0.70      0.70        61\n",
      "weighted avg       0.70      0.70      0.70        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train,y_train)\n",
    "tree_pred = decision_tree.predict(X_test)\n",
    "DTC_score_heart = precision_score(y_test,tree_pred)\n",
    "print(confusion_matrix(y_test,tree_pred))\n",
    "print(classification_report(y_test,tree_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22  7]\n",
      " [ 8 24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75        29\n",
      "           1       0.77      0.75      0.76        32\n",
      "\n",
      "    accuracy                           0.75        61\n",
      "   macro avg       0.75      0.75      0.75        61\n",
      "weighted avg       0.75      0.75      0.75        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "RFC_score_heart = precision_score(y_test,rf_pred)\n",
    "print(confusion_matrix(y_test,rf_pred))\n",
    "print(classification_report(y_test,rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24  5]\n",
      " [ 9 23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.77        29\n",
      "           1       0.82      0.72      0.77        32\n",
      "\n",
      "    accuracy                           0.77        61\n",
      "   macro avg       0.77      0.77      0.77        61\n",
      "weighted avg       0.78      0.77      0.77        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KN = KNeighborsClassifier()\n",
    "KN.fit(X_train,y_train)\n",
    "KN_pred = KN.predict(X_test)\n",
    "KN_score_heart = precision_score(y_test,KN_pred)\n",
    "print(confusion_matrix(y_test,KN_pred))\n",
    "print(classification_report(y_test,KN_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 222ms/step - loss: 0.6959 - accuracy: 0.5159 - val_loss: 0.6845 - val_accuracy: 0.5529\n",
      "Epoch 2/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6952 - accuracy: 0.5478 - val_loss: 0.6838 - val_accuracy: 0.5529\n",
      "Epoch 3/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6943 - accuracy: 0.5478 - val_loss: 0.6830 - val_accuracy: 0.5529\n",
      "Epoch 4/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6934 - accuracy: 0.5478 - val_loss: 0.6821 - val_accuracy: 0.5529\n",
      "Epoch 5/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6923 - accuracy: 0.5478 - val_loss: 0.6812 - val_accuracy: 0.5529\n",
      "Epoch 6/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6913 - accuracy: 0.5478 - val_loss: 0.6802 - val_accuracy: 0.5529\n",
      "Epoch 7/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6900 - accuracy: 0.5478 - val_loss: 0.6793 - val_accuracy: 0.5529\n",
      "Epoch 8/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6889 - accuracy: 0.5478 - val_loss: 0.6785 - val_accuracy: 0.5529\n",
      "Epoch 9/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6874 - accuracy: 0.5478 - val_loss: 0.6777 - val_accuracy: 0.5529\n",
      "Epoch 10/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6858 - accuracy: 0.5478 - val_loss: 0.6772 - val_accuracy: 0.5529\n",
      "Epoch 11/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6840 - accuracy: 0.5478 - val_loss: 0.6767 - val_accuracy: 0.5529\n",
      "Epoch 12/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6822 - accuracy: 0.5478 - val_loss: 0.6763 - val_accuracy: 0.5529\n",
      "Epoch 13/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6796 - accuracy: 0.5478 - val_loss: 0.6759 - val_accuracy: 0.5529\n",
      "Epoch 14/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6772 - accuracy: 0.5478 - val_loss: 0.6751 - val_accuracy: 0.5529\n",
      "Epoch 15/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6747 - accuracy: 0.5478 - val_loss: 0.6740 - val_accuracy: 0.5529\n",
      "Epoch 16/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6725 - accuracy: 0.5478 - val_loss: 0.6727 - val_accuracy: 0.5529\n",
      "Epoch 17/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6704 - accuracy: 0.5478 - val_loss: 0.6713 - val_accuracy: 0.5529\n",
      "Epoch 18/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6684 - accuracy: 0.5478 - val_loss: 0.6700 - val_accuracy: 0.5529\n",
      "Epoch 19/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6664 - accuracy: 0.5478 - val_loss: 0.6687 - val_accuracy: 0.5529\n",
      "Epoch 20/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6645 - accuracy: 0.5478 - val_loss: 0.6676 - val_accuracy: 0.5529\n",
      "Epoch 21/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6627 - accuracy: 0.5478 - val_loss: 0.6665 - val_accuracy: 0.5529\n",
      "Epoch 22/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6606 - accuracy: 0.5478 - val_loss: 0.6658 - val_accuracy: 0.5529\n",
      "Epoch 23/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6591 - accuracy: 0.5478 - val_loss: 0.6652 - val_accuracy: 0.5529\n",
      "Epoch 24/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6571 - accuracy: 0.5478 - val_loss: 0.6644 - val_accuracy: 0.5529\n",
      "Epoch 25/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6557 - accuracy: 0.5478 - val_loss: 0.6636 - val_accuracy: 0.5529\n",
      "Epoch 26/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6541 - accuracy: 0.5478 - val_loss: 0.6626 - val_accuracy: 0.5529\n",
      "Epoch 27/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6528 - accuracy: 0.5478 - val_loss: 0.6616 - val_accuracy: 0.5529\n",
      "Epoch 28/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6514 - accuracy: 0.5478 - val_loss: 0.6607 - val_accuracy: 0.5529\n",
      "Epoch 29/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6499 - accuracy: 0.5478 - val_loss: 0.6599 - val_accuracy: 0.5529\n",
      "Epoch 30/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6486 - accuracy: 0.5478 - val_loss: 0.6591 - val_accuracy: 0.5529\n",
      "Epoch 31/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6473 - accuracy: 0.5478 - val_loss: 0.6582 - val_accuracy: 0.5529\n",
      "Epoch 32/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6459 - accuracy: 0.5478 - val_loss: 0.6572 - val_accuracy: 0.5529\n",
      "Epoch 33/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6446 - accuracy: 0.5478 - val_loss: 0.6562 - val_accuracy: 0.5529\n",
      "Epoch 34/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6433 - accuracy: 0.5478 - val_loss: 0.6551 - val_accuracy: 0.5529\n",
      "Epoch 35/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6420 - accuracy: 0.5478 - val_loss: 0.6541 - val_accuracy: 0.5529\n",
      "Epoch 36/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6407 - accuracy: 0.5478 - val_loss: 0.6530 - val_accuracy: 0.5529\n",
      "Epoch 37/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6394 - accuracy: 0.5478 - val_loss: 0.6520 - val_accuracy: 0.5529\n",
      "Epoch 38/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6380 - accuracy: 0.5478 - val_loss: 0.6510 - val_accuracy: 0.5529\n",
      "Epoch 39/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6367 - accuracy: 0.5478 - val_loss: 0.6502 - val_accuracy: 0.5529\n",
      "Epoch 40/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6354 - accuracy: 0.5478 - val_loss: 0.6494 - val_accuracy: 0.5529\n",
      "Epoch 41/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6339 - accuracy: 0.5478 - val_loss: 0.6487 - val_accuracy: 0.5529\n",
      "Epoch 42/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6326 - accuracy: 0.5478 - val_loss: 0.6479 - val_accuracy: 0.5529\n",
      "Epoch 43/400\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6311 - accuracy: 0.5478 - val_loss: 0.6472 - val_accuracy: 0.5529\n",
      "Epoch 44/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6295 - accuracy: 0.5478 - val_loss: 0.6464 - val_accuracy: 0.5529\n",
      "Epoch 45/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6279 - accuracy: 0.5478 - val_loss: 0.6457 - val_accuracy: 0.5529\n",
      "Epoch 46/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6261 - accuracy: 0.5478 - val_loss: 0.6451 - val_accuracy: 0.5529\n",
      "Epoch 47/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6244 - accuracy: 0.5478 - val_loss: 0.6445 - val_accuracy: 0.5529\n",
      "Epoch 48/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6229 - accuracy: 0.5478 - val_loss: 0.6437 - val_accuracy: 0.5529\n",
      "Epoch 49/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6210 - accuracy: 0.5796 - val_loss: 0.6426 - val_accuracy: 0.6118\n",
      "Epoch 50/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6195 - accuracy: 0.7261 - val_loss: 0.6415 - val_accuracy: 0.6118\n",
      "Epoch 51/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6178 - accuracy: 0.7261 - val_loss: 0.6405 - val_accuracy: 0.6118\n",
      "Epoch 52/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6162 - accuracy: 0.7389 - val_loss: 0.6397 - val_accuracy: 0.6118\n",
      "Epoch 53/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6146 - accuracy: 0.7389 - val_loss: 0.6389 - val_accuracy: 0.6118\n",
      "Epoch 54/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6130 - accuracy: 0.7261 - val_loss: 0.6379 - val_accuracy: 0.6118\n",
      "Epoch 55/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6113 - accuracy: 0.7389 - val_loss: 0.6369 - val_accuracy: 0.6118\n",
      "Epoch 56/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6098 - accuracy: 0.7389 - val_loss: 0.6360 - val_accuracy: 0.6118\n",
      "Epoch 57/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6081 - accuracy: 0.7389 - val_loss: 0.6349 - val_accuracy: 0.6235\n",
      "Epoch 58/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6065 - accuracy: 0.7516 - val_loss: 0.6335 - val_accuracy: 0.6235\n",
      "Epoch 59/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6048 - accuracy: 0.7580 - val_loss: 0.6323 - val_accuracy: 0.6235\n",
      "Epoch 60/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6032 - accuracy: 0.7643 - val_loss: 0.6313 - val_accuracy: 0.6235\n",
      "Epoch 61/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6015 - accuracy: 0.7643 - val_loss: 0.6303 - val_accuracy: 0.6353\n",
      "Epoch 62/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5999 - accuracy: 0.7643 - val_loss: 0.6291 - val_accuracy: 0.6235\n",
      "Epoch 63/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5983 - accuracy: 0.7643 - val_loss: 0.6279 - val_accuracy: 0.6235\n",
      "Epoch 64/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5967 - accuracy: 0.7643 - val_loss: 0.6267 - val_accuracy: 0.6235\n",
      "Epoch 65/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5952 - accuracy: 0.7643 - val_loss: 0.6255 - val_accuracy: 0.6353\n",
      "Epoch 66/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5937 - accuracy: 0.7643 - val_loss: 0.6243 - val_accuracy: 0.6353\n",
      "Epoch 67/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5922 - accuracy: 0.7707 - val_loss: 0.6233 - val_accuracy: 0.6353\n",
      "Epoch 68/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5908 - accuracy: 0.7707 - val_loss: 0.6224 - val_accuracy: 0.6353\n",
      "Epoch 69/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5893 - accuracy: 0.7580 - val_loss: 0.6215 - val_accuracy: 0.6353\n",
      "Epoch 70/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5878 - accuracy: 0.7580 - val_loss: 0.6205 - val_accuracy: 0.6353\n",
      "Epoch 71/400\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5864 - accuracy: 0.7580 - val_loss: 0.6194 - val_accuracy: 0.6353\n",
      "Epoch 72/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5851 - accuracy: 0.7580 - val_loss: 0.6182 - val_accuracy: 0.6353\n",
      "Epoch 73/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5836 - accuracy: 0.7580 - val_loss: 0.6173 - val_accuracy: 0.6353\n",
      "Epoch 74/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5824 - accuracy: 0.7580 - val_loss: 0.6163 - val_accuracy: 0.6353\n",
      "Epoch 75/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5809 - accuracy: 0.7580 - val_loss: 0.6154 - val_accuracy: 0.6353\n",
      "Epoch 76/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5795 - accuracy: 0.7580 - val_loss: 0.6147 - val_accuracy: 0.6353\n",
      "Epoch 77/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5782 - accuracy: 0.7580 - val_loss: 0.6140 - val_accuracy: 0.6471\n",
      "Epoch 78/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5769 - accuracy: 0.7580 - val_loss: 0.6136 - val_accuracy: 0.6471\n",
      "Epoch 79/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5757 - accuracy: 0.7580 - val_loss: 0.6129 - val_accuracy: 0.6471\n",
      "Epoch 80/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5745 - accuracy: 0.7580 - val_loss: 0.6122 - val_accuracy: 0.6471\n",
      "Epoch 81/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5732 - accuracy: 0.7580 - val_loss: 0.6114 - val_accuracy: 0.6471\n",
      "Epoch 82/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5721 - accuracy: 0.7580 - val_loss: 0.6105 - val_accuracy: 0.6471\n",
      "Epoch 83/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5707 - accuracy: 0.7580 - val_loss: 0.6093 - val_accuracy: 0.6588\n",
      "Epoch 84/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5694 - accuracy: 0.7643 - val_loss: 0.6079 - val_accuracy: 0.6588\n",
      "Epoch 85/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5680 - accuracy: 0.7643 - val_loss: 0.6066 - val_accuracy: 0.6588\n",
      "Epoch 86/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5666 - accuracy: 0.7707 - val_loss: 0.6054 - val_accuracy: 0.6588\n",
      "Epoch 87/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5652 - accuracy: 0.7771 - val_loss: 0.6042 - val_accuracy: 0.6706\n",
      "Epoch 88/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5639 - accuracy: 0.7771 - val_loss: 0.6029 - val_accuracy: 0.6706\n",
      "Epoch 89/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5626 - accuracy: 0.7771 - val_loss: 0.6015 - val_accuracy: 0.6706\n",
      "Epoch 90/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5613 - accuracy: 0.7771 - val_loss: 0.6003 - val_accuracy: 0.6588\n",
      "Epoch 91/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5600 - accuracy: 0.7771 - val_loss: 0.5992 - val_accuracy: 0.6588\n",
      "Epoch 92/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5587 - accuracy: 0.7771 - val_loss: 0.5981 - val_accuracy: 0.6588\n",
      "Epoch 93/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5575 - accuracy: 0.7771 - val_loss: 0.5969 - val_accuracy: 0.6588\n",
      "Epoch 94/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5561 - accuracy: 0.7707 - val_loss: 0.5958 - val_accuracy: 0.6706\n",
      "Epoch 95/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5548 - accuracy: 0.7707 - val_loss: 0.5945 - val_accuracy: 0.6706\n",
      "Epoch 96/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5536 - accuracy: 0.7771 - val_loss: 0.5934 - val_accuracy: 0.6706\n",
      "Epoch 97/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5523 - accuracy: 0.7898 - val_loss: 0.5924 - val_accuracy: 0.6706\n",
      "Epoch 98/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5510 - accuracy: 0.7898 - val_loss: 0.5917 - val_accuracy: 0.6824\n",
      "Epoch 99/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5498 - accuracy: 0.7834 - val_loss: 0.5909 - val_accuracy: 0.6824\n",
      "Epoch 100/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5485 - accuracy: 0.7834 - val_loss: 0.5899 - val_accuracy: 0.6824\n",
      "Epoch 101/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5473 - accuracy: 0.7898 - val_loss: 0.5887 - val_accuracy: 0.6824\n",
      "Epoch 102/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5460 - accuracy: 0.7898 - val_loss: 0.5877 - val_accuracy: 0.6824\n",
      "Epoch 103/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5448 - accuracy: 0.7898 - val_loss: 0.5868 - val_accuracy: 0.6824\n",
      "Epoch 104/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5437 - accuracy: 0.7898 - val_loss: 0.5858 - val_accuracy: 0.6824\n",
      "Epoch 105/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5424 - accuracy: 0.7898 - val_loss: 0.5846 - val_accuracy: 0.6824\n",
      "Epoch 106/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5413 - accuracy: 0.7898 - val_loss: 0.5835 - val_accuracy: 0.6824\n",
      "Epoch 107/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5400 - accuracy: 0.7898 - val_loss: 0.5821 - val_accuracy: 0.6824\n",
      "Epoch 108/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5387 - accuracy: 0.7898 - val_loss: 0.5806 - val_accuracy: 0.6824\n",
      "Epoch 109/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5376 - accuracy: 0.7962 - val_loss: 0.5791 - val_accuracy: 0.6824\n",
      "Epoch 110/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5363 - accuracy: 0.7962 - val_loss: 0.5775 - val_accuracy: 0.6824\n",
      "Epoch 111/400\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5349 - accuracy: 0.7962 - val_loss: 0.5758 - val_accuracy: 0.6706\n",
      "Epoch 112/400\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5336 - accuracy: 0.7962 - val_loss: 0.5742 - val_accuracy: 0.6706\n",
      "Epoch 113/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5322 - accuracy: 0.7962 - val_loss: 0.5727 - val_accuracy: 0.6706\n",
      "Epoch 114/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5308 - accuracy: 0.7962 - val_loss: 0.5711 - val_accuracy: 0.6706\n",
      "Epoch 115/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5294 - accuracy: 0.8025 - val_loss: 0.5697 - val_accuracy: 0.6706\n",
      "Epoch 116/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5277 - accuracy: 0.8025 - val_loss: 0.5683 - val_accuracy: 0.6706\n",
      "Epoch 117/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5262 - accuracy: 0.8025 - val_loss: 0.5670 - val_accuracy: 0.6824\n",
      "Epoch 118/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5248 - accuracy: 0.8025 - val_loss: 0.5659 - val_accuracy: 0.6824\n",
      "Epoch 119/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5232 - accuracy: 0.8025 - val_loss: 0.5645 - val_accuracy: 0.6824\n",
      "Epoch 120/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5219 - accuracy: 0.8025 - val_loss: 0.5633 - val_accuracy: 0.6824\n",
      "Epoch 121/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5204 - accuracy: 0.8025 - val_loss: 0.5621 - val_accuracy: 0.6824\n",
      "Epoch 122/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5190 - accuracy: 0.8089 - val_loss: 0.5609 - val_accuracy: 0.6706\n",
      "Epoch 123/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5178 - accuracy: 0.8089 - val_loss: 0.5598 - val_accuracy: 0.6706\n",
      "Epoch 124/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5166 - accuracy: 0.8089 - val_loss: 0.5588 - val_accuracy: 0.6706\n",
      "Epoch 125/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5153 - accuracy: 0.8089 - val_loss: 0.5578 - val_accuracy: 0.6706\n",
      "Epoch 126/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5141 - accuracy: 0.8089 - val_loss: 0.5570 - val_accuracy: 0.6706\n",
      "Epoch 127/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5129 - accuracy: 0.8089 - val_loss: 0.5564 - val_accuracy: 0.6706\n",
      "Epoch 128/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5116 - accuracy: 0.8089 - val_loss: 0.5559 - val_accuracy: 0.6706\n",
      "Epoch 129/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5103 - accuracy: 0.8089 - val_loss: 0.5556 - val_accuracy: 0.6706\n",
      "Epoch 130/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5090 - accuracy: 0.8089 - val_loss: 0.5553 - val_accuracy: 0.6824\n",
      "Epoch 131/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5079 - accuracy: 0.8089 - val_loss: 0.5548 - val_accuracy: 0.6824\n",
      "Epoch 132/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5068 - accuracy: 0.8089 - val_loss: 0.5541 - val_accuracy: 0.6941\n",
      "Epoch 133/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5056 - accuracy: 0.8153 - val_loss: 0.5529 - val_accuracy: 0.6824\n",
      "Epoch 134/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5044 - accuracy: 0.8153 - val_loss: 0.5515 - val_accuracy: 0.6824\n",
      "Epoch 135/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5033 - accuracy: 0.8153 - val_loss: 0.5500 - val_accuracy: 0.6824\n",
      "Epoch 136/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5021 - accuracy: 0.8217 - val_loss: 0.5487 - val_accuracy: 0.6824\n",
      "Epoch 137/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5009 - accuracy: 0.8344 - val_loss: 0.5476 - val_accuracy: 0.6824\n",
      "Epoch 138/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4999 - accuracy: 0.8344 - val_loss: 0.5465 - val_accuracy: 0.6824\n",
      "Epoch 139/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4987 - accuracy: 0.8344 - val_loss: 0.5450 - val_accuracy: 0.6941\n",
      "Epoch 140/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4976 - accuracy: 0.8344 - val_loss: 0.5435 - val_accuracy: 0.6941\n",
      "Epoch 141/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4966 - accuracy: 0.8344 - val_loss: 0.5420 - val_accuracy: 0.6941\n",
      "Epoch 142/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4956 - accuracy: 0.8408 - val_loss: 0.5404 - val_accuracy: 0.6941\n",
      "Epoch 143/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4946 - accuracy: 0.8599 - val_loss: 0.5391 - val_accuracy: 0.6941\n",
      "Epoch 144/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4935 - accuracy: 0.8535 - val_loss: 0.5384 - val_accuracy: 0.6941\n",
      "Epoch 145/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4926 - accuracy: 0.8535 - val_loss: 0.5380 - val_accuracy: 0.6941\n",
      "Epoch 146/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4911 - accuracy: 0.8535 - val_loss: 0.5371 - val_accuracy: 0.6941\n",
      "Epoch 147/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4900 - accuracy: 0.8535 - val_loss: 0.5367 - val_accuracy: 0.6941\n",
      "Epoch 148/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4888 - accuracy: 0.8535 - val_loss: 0.5363 - val_accuracy: 0.6941\n",
      "Epoch 149/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4874 - accuracy: 0.8535 - val_loss: 0.5359 - val_accuracy: 0.6941\n",
      "Epoch 150/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4863 - accuracy: 0.8535 - val_loss: 0.5357 - val_accuracy: 0.6941\n",
      "Epoch 151/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4850 - accuracy: 0.8535 - val_loss: 0.5354 - val_accuracy: 0.6941\n",
      "Epoch 152/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4837 - accuracy: 0.8535 - val_loss: 0.5352 - val_accuracy: 0.6941\n",
      "Epoch 153/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4824 - accuracy: 0.8535 - val_loss: 0.5346 - val_accuracy: 0.6941\n",
      "Epoch 154/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4810 - accuracy: 0.8535 - val_loss: 0.5338 - val_accuracy: 0.6941\n",
      "Epoch 155/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4797 - accuracy: 0.8535 - val_loss: 0.5334 - val_accuracy: 0.6941\n",
      "Epoch 156/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4784 - accuracy: 0.8535 - val_loss: 0.5333 - val_accuracy: 0.6941\n",
      "Epoch 157/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4771 - accuracy: 0.8535 - val_loss: 0.5333 - val_accuracy: 0.6941\n",
      "Epoch 158/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4763 - accuracy: 0.8535 - val_loss: 0.5329 - val_accuracy: 0.6941\n",
      "Epoch 159/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4749 - accuracy: 0.8535 - val_loss: 0.5317 - val_accuracy: 0.6941\n",
      "Epoch 160/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4739 - accuracy: 0.8535 - val_loss: 0.5306 - val_accuracy: 0.6941\n",
      "Epoch 161/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4727 - accuracy: 0.8535 - val_loss: 0.5300 - val_accuracy: 0.6941\n",
      "Epoch 162/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4717 - accuracy: 0.8662 - val_loss: 0.5294 - val_accuracy: 0.6941\n",
      "Epoch 163/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4707 - accuracy: 0.8662 - val_loss: 0.5287 - val_accuracy: 0.6941\n",
      "Epoch 164/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4698 - accuracy: 0.8726 - val_loss: 0.5280 - val_accuracy: 0.6941\n",
      "Epoch 165/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4688 - accuracy: 0.8726 - val_loss: 0.5268 - val_accuracy: 0.6941\n",
      "Epoch 166/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4679 - accuracy: 0.8726 - val_loss: 0.5257 - val_accuracy: 0.7059\n",
      "Epoch 167/400\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4672 - accuracy: 0.8726 - val_loss: 0.5247 - val_accuracy: 0.7176\n",
      "Epoch 168/400\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4664 - accuracy: 0.8726 - val_loss: 0.5238 - val_accuracy: 0.7176\n",
      "Epoch 169/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4656 - accuracy: 0.8726 - val_loss: 0.5225 - val_accuracy: 0.7176\n",
      "Epoch 170/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4651 - accuracy: 0.8790 - val_loss: 0.5213 - val_accuracy: 0.7176\n",
      "Epoch 171/400\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4646 - accuracy: 0.8790 - val_loss: 0.5205 - val_accuracy: 0.7176\n",
      "Epoch 172/400\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4639 - accuracy: 0.8790 - val_loss: 0.5201 - val_accuracy: 0.7176\n",
      "Epoch 173/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4631 - accuracy: 0.8790 - val_loss: 0.5197 - val_accuracy: 0.7176\n",
      "Epoch 174/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4624 - accuracy: 0.8790 - val_loss: 0.5195 - val_accuracy: 0.7176\n",
      "Epoch 175/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4617 - accuracy: 0.8790 - val_loss: 0.5190 - val_accuracy: 0.7176\n",
      "Epoch 176/400\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4610 - accuracy: 0.8790 - val_loss: 0.5184 - val_accuracy: 0.7176\n",
      "Epoch 177/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4603 - accuracy: 0.8790 - val_loss: 0.5183 - val_accuracy: 0.7176\n",
      "Epoch 178/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4597 - accuracy: 0.8790 - val_loss: 0.5185 - val_accuracy: 0.7176\n",
      "Epoch 179/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4588 - accuracy: 0.8790 - val_loss: 0.5190 - val_accuracy: 0.7176\n",
      "Epoch 180/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4581 - accuracy: 0.8790 - val_loss: 0.5198 - val_accuracy: 0.7176\n",
      "Epoch 181/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4573 - accuracy: 0.8790 - val_loss: 0.5207 - val_accuracy: 0.7176\n",
      "Epoch 182/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4567 - accuracy: 0.8790 - val_loss: 0.5211 - val_accuracy: 0.7176\n",
      "Epoch 183/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4559 - accuracy: 0.8790 - val_loss: 0.5209 - val_accuracy: 0.7176\n",
      "Epoch 184/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4552 - accuracy: 0.8790 - val_loss: 0.5210 - val_accuracy: 0.7176\n",
      "Epoch 185/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4547 - accuracy: 0.8790 - val_loss: 0.5210 - val_accuracy: 0.7176\n",
      "Epoch 186/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4540 - accuracy: 0.8790 - val_loss: 0.5199 - val_accuracy: 0.7176\n",
      "Epoch 187/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4533 - accuracy: 0.8790 - val_loss: 0.5181 - val_accuracy: 0.7176\n",
      "Epoch 188/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4530 - accuracy: 0.8790 - val_loss: 0.5170 - val_accuracy: 0.7176\n",
      "Epoch 189/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4524 - accuracy: 0.8790 - val_loss: 0.5166 - val_accuracy: 0.7176\n",
      "Epoch 190/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4518 - accuracy: 0.8790 - val_loss: 0.5168 - val_accuracy: 0.7176\n",
      "Epoch 191/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4511 - accuracy: 0.8790 - val_loss: 0.5175 - val_accuracy: 0.7176\n",
      "Epoch 192/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4504 - accuracy: 0.8790 - val_loss: 0.5187 - val_accuracy: 0.7176\n",
      "Epoch 193/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4499 - accuracy: 0.8790 - val_loss: 0.5202 - val_accuracy: 0.7176\n",
      "Epoch 194/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4494 - accuracy: 0.8790 - val_loss: 0.5210 - val_accuracy: 0.7176\n",
      "Epoch 195/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4488 - accuracy: 0.8790 - val_loss: 0.5214 - val_accuracy: 0.7176\n",
      "Epoch 196/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4484 - accuracy: 0.8790 - val_loss: 0.5219 - val_accuracy: 0.7176\n",
      "Epoch 197/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4480 - accuracy: 0.8790 - val_loss: 0.5222 - val_accuracy: 0.7176\n",
      "Epoch 198/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4476 - accuracy: 0.8790 - val_loss: 0.5216 - val_accuracy: 0.7176\n",
      "Epoch 199/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4469 - accuracy: 0.8790 - val_loss: 0.5200 - val_accuracy: 0.7176\n",
      "Epoch 200/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4463 - accuracy: 0.8790 - val_loss: 0.5183 - val_accuracy: 0.7176\n",
      "Epoch 201/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4458 - accuracy: 0.8790 - val_loss: 0.5170 - val_accuracy: 0.7176\n",
      "Epoch 202/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4454 - accuracy: 0.8790 - val_loss: 0.5162 - val_accuracy: 0.7176\n",
      "Epoch 203/400\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4450 - accuracy: 0.8790 - val_loss: 0.5164 - val_accuracy: 0.7176\n",
      "Epoch 204/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4444 - accuracy: 0.8790 - val_loss: 0.5172 - val_accuracy: 0.7176\n",
      "Epoch 205/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4439 - accuracy: 0.8790 - val_loss: 0.5177 - val_accuracy: 0.7176\n",
      "Epoch 206/400\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4434 - accuracy: 0.8790 - val_loss: 0.5182 - val_accuracy: 0.7176\n",
      "Epoch 207/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4429 - accuracy: 0.8790 - val_loss: 0.5192 - val_accuracy: 0.7176\n",
      "Epoch 208/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4424 - accuracy: 0.8790 - val_loss: 0.5207 - val_accuracy: 0.7176\n",
      "Epoch 209/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4423 - accuracy: 0.8790 - val_loss: 0.5217 - val_accuracy: 0.7059\n",
      "Epoch 210/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4419 - accuracy: 0.8790 - val_loss: 0.5218 - val_accuracy: 0.7059\n",
      "Epoch 211/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4415 - accuracy: 0.8790 - val_loss: 0.5217 - val_accuracy: 0.7059\n",
      "Epoch 212/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4411 - accuracy: 0.8790 - val_loss: 0.5206 - val_accuracy: 0.7176\n",
      "Epoch 213/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4403 - accuracy: 0.8790 - val_loss: 0.5180 - val_accuracy: 0.7176\n",
      "Epoch 214/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4395 - accuracy: 0.8790 - val_loss: 0.5148 - val_accuracy: 0.7176\n",
      "Epoch 215/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4391 - accuracy: 0.8726 - val_loss: 0.5118 - val_accuracy: 0.7176\n",
      "Epoch 216/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4390 - accuracy: 0.8726 - val_loss: 0.5093 - val_accuracy: 0.7294\n",
      "Epoch 217/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4392 - accuracy: 0.8726 - val_loss: 0.5079 - val_accuracy: 0.7294\n",
      "Epoch 218/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4389 - accuracy: 0.8726 - val_loss: 0.5076 - val_accuracy: 0.7294\n",
      "Epoch 219/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4385 - accuracy: 0.8726 - val_loss: 0.5076 - val_accuracy: 0.7294\n",
      "Epoch 220/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4380 - accuracy: 0.8726 - val_loss: 0.5080 - val_accuracy: 0.7294\n",
      "Epoch 221/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4372 - accuracy: 0.8726 - val_loss: 0.5096 - val_accuracy: 0.7176\n",
      "Epoch 222/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4363 - accuracy: 0.8726 - val_loss: 0.5119 - val_accuracy: 0.7176\n",
      "Epoch 223/400\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4362 - accuracy: 0.8726 - val_loss: 0.5144 - val_accuracy: 0.7176\n",
      "Epoch 224/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4354 - accuracy: 0.8726 - val_loss: 0.5158 - val_accuracy: 0.7176\n",
      "Epoch 225/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4356 - accuracy: 0.8726 - val_loss: 0.5162 - val_accuracy: 0.7176\n",
      "Epoch 226/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4348 - accuracy: 0.8726 - val_loss: 0.5150 - val_accuracy: 0.7176\n",
      "Epoch 227/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4343 - accuracy: 0.8726 - val_loss: 0.5142 - val_accuracy: 0.7176\n",
      "Epoch 228/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4340 - accuracy: 0.8726 - val_loss: 0.5128 - val_accuracy: 0.7176\n",
      "Epoch 229/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4333 - accuracy: 0.8726 - val_loss: 0.5100 - val_accuracy: 0.7176\n",
      "Epoch 230/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4330 - accuracy: 0.8726 - val_loss: 0.5075 - val_accuracy: 0.7176\n",
      "Epoch 231/400\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4328 - accuracy: 0.8726 - val_loss: 0.5056 - val_accuracy: 0.7294\n",
      "Epoch 232/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4326 - accuracy: 0.8662 - val_loss: 0.5045 - val_accuracy: 0.7294\n",
      "Epoch 233/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4324 - accuracy: 0.8662 - val_loss: 0.5035 - val_accuracy: 0.7176\n",
      "Epoch 234/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4323 - accuracy: 0.8662 - val_loss: 0.5029 - val_accuracy: 0.7059\n",
      "Epoch 235/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4319 - accuracy: 0.8662 - val_loss: 0.5031 - val_accuracy: 0.7059\n",
      "Epoch 236/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4314 - accuracy: 0.8662 - val_loss: 0.5038 - val_accuracy: 0.7176\n",
      "Epoch 237/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4307 - accuracy: 0.8662 - val_loss: 0.5053 - val_accuracy: 0.7176\n",
      "Epoch 238/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4299 - accuracy: 0.8662 - val_loss: 0.5075 - val_accuracy: 0.7176\n",
      "Epoch 239/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4295 - accuracy: 0.8726 - val_loss: 0.5100 - val_accuracy: 0.7176\n",
      "Epoch 240/400\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4289 - accuracy: 0.8726 - val_loss: 0.5124 - val_accuracy: 0.7176\n",
      "Epoch 241/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4288 - accuracy: 0.8726 - val_loss: 0.5155 - val_accuracy: 0.7294\n",
      "Epoch 242/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4291 - accuracy: 0.8726 - val_loss: 0.5178 - val_accuracy: 0.7294\n",
      "Epoch 243/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4289 - accuracy: 0.8726 - val_loss: 0.5187 - val_accuracy: 0.7294\n",
      "Epoch 244/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4288 - accuracy: 0.8726 - val_loss: 0.5188 - val_accuracy: 0.7294\n",
      "Epoch 245/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4284 - accuracy: 0.8726 - val_loss: 0.5179 - val_accuracy: 0.7294\n",
      "Epoch 246/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4278 - accuracy: 0.8726 - val_loss: 0.5163 - val_accuracy: 0.7294\n",
      "Epoch 247/400\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4272 - accuracy: 0.8726 - val_loss: 0.5142 - val_accuracy: 0.7294\n",
      "Epoch 248/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4264 - accuracy: 0.8726 - val_loss: 0.5114 - val_accuracy: 0.7176\n",
      "Epoch 249/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4261 - accuracy: 0.8726 - val_loss: 0.5081 - val_accuracy: 0.7176\n",
      "Epoch 250/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4252 - accuracy: 0.8726 - val_loss: 0.5054 - val_accuracy: 0.7176\n",
      "Epoch 251/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4249 - accuracy: 0.8662 - val_loss: 0.5027 - val_accuracy: 0.7294\n",
      "Epoch 252/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4247 - accuracy: 0.8662 - val_loss: 0.5001 - val_accuracy: 0.7176\n",
      "Epoch 253/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4250 - accuracy: 0.8726 - val_loss: 0.4982 - val_accuracy: 0.7176\n",
      "Epoch 254/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4254 - accuracy: 0.8726 - val_loss: 0.4969 - val_accuracy: 0.7176\n",
      "Epoch 255/400\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4255 - accuracy: 0.8726 - val_loss: 0.4966 - val_accuracy: 0.7176\n",
      "Epoch 256/400\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4251 - accuracy: 0.8726 - val_loss: 0.4970 - val_accuracy: 0.7176\n",
      "Epoch 257/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4243 - accuracy: 0.8726 - val_loss: 0.4979 - val_accuracy: 0.7176\n",
      "Epoch 258/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4236 - accuracy: 0.8726 - val_loss: 0.4993 - val_accuracy: 0.7176\n",
      "Epoch 259/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4227 - accuracy: 0.8726 - val_loss: 0.5007 - val_accuracy: 0.7176\n",
      "Epoch 260/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4222 - accuracy: 0.8662 - val_loss: 0.5018 - val_accuracy: 0.7176\n",
      "Epoch 261/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4216 - accuracy: 0.8662 - val_loss: 0.5023 - val_accuracy: 0.7059\n",
      "Epoch 262/400\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4211 - accuracy: 0.8662 - val_loss: 0.5027 - val_accuracy: 0.7059\n",
      "Epoch 263/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4209 - accuracy: 0.8662 - val_loss: 0.5028 - val_accuracy: 0.7059\n",
      "Epoch 264/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4204 - accuracy: 0.8662 - val_loss: 0.5024 - val_accuracy: 0.7059\n",
      "Epoch 265/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4201 - accuracy: 0.8662 - val_loss: 0.5021 - val_accuracy: 0.7059\n",
      "Epoch 266/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4197 - accuracy: 0.8662 - val_loss: 0.5020 - val_accuracy: 0.7059\n",
      "Epoch 267/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4194 - accuracy: 0.8662 - val_loss: 0.5023 - val_accuracy: 0.7059\n",
      "Epoch 268/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4190 - accuracy: 0.8662 - val_loss: 0.5035 - val_accuracy: 0.7059\n",
      "Epoch 269/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4185 - accuracy: 0.8662 - val_loss: 0.5049 - val_accuracy: 0.7294\n",
      "Epoch 270/400\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4186 - accuracy: 0.8726 - val_loss: 0.5065 - val_accuracy: 0.7294\n",
      "Epoch 271/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4182 - accuracy: 0.8662 - val_loss: 0.5066 - val_accuracy: 0.7294\n",
      "Epoch 272/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4179 - accuracy: 0.8662 - val_loss: 0.5064 - val_accuracy: 0.7294\n",
      "Epoch 273/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4175 - accuracy: 0.8662 - val_loss: 0.5061 - val_accuracy: 0.7294\n",
      "Epoch 274/400\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4171 - accuracy: 0.8726 - val_loss: 0.5048 - val_accuracy: 0.7294\n",
      "Epoch 275/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4168 - accuracy: 0.8726 - val_loss: 0.5037 - val_accuracy: 0.7294\n",
      "Epoch 276/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4163 - accuracy: 0.8726 - val_loss: 0.5031 - val_accuracy: 0.7294\n",
      "Epoch 277/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4159 - accuracy: 0.8662 - val_loss: 0.5022 - val_accuracy: 0.7294\n",
      "Epoch 278/400\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4155 - accuracy: 0.8662 - val_loss: 0.5008 - val_accuracy: 0.7294\n",
      "Epoch 279/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4152 - accuracy: 0.8662 - val_loss: 0.4996 - val_accuracy: 0.7176\n",
      "Epoch 280/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4148 - accuracy: 0.8662 - val_loss: 0.4986 - val_accuracy: 0.7176\n",
      "Epoch 281/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4146 - accuracy: 0.8662 - val_loss: 0.4976 - val_accuracy: 0.7059\n",
      "Epoch 282/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4143 - accuracy: 0.8662 - val_loss: 0.4975 - val_accuracy: 0.7059\n",
      "Epoch 283/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4139 - accuracy: 0.8662 - val_loss: 0.4978 - val_accuracy: 0.7176\n",
      "Epoch 284/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4135 - accuracy: 0.8662 - val_loss: 0.4985 - val_accuracy: 0.7294\n",
      "Epoch 285/400\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.4131 - accuracy: 0.8662 - val_loss: 0.4994 - val_accuracy: 0.7294\n",
      "Epoch 286/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4132 - accuracy: 0.8662 - val_loss: 0.4996 - val_accuracy: 0.7294\n",
      "Epoch 287/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4125 - accuracy: 0.8662 - val_loss: 0.4987 - val_accuracy: 0.7294\n",
      "Epoch 288/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4121 - accuracy: 0.8662 - val_loss: 0.4983 - val_accuracy: 0.7294\n",
      "Epoch 289/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4118 - accuracy: 0.8662 - val_loss: 0.4976 - val_accuracy: 0.7176\n",
      "Epoch 290/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4115 - accuracy: 0.8662 - val_loss: 0.4968 - val_accuracy: 0.7176\n",
      "Epoch 291/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4113 - accuracy: 0.8662 - val_loss: 0.4965 - val_accuracy: 0.7176\n",
      "Epoch 292/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4109 - accuracy: 0.8662 - val_loss: 0.4967 - val_accuracy: 0.7176\n",
      "Epoch 293/400\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4106 - accuracy: 0.8662 - val_loss: 0.4971 - val_accuracy: 0.7176\n",
      "Epoch 294/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4103 - accuracy: 0.8662 - val_loss: 0.4969 - val_accuracy: 0.7176\n",
      "Epoch 295/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4099 - accuracy: 0.8662 - val_loss: 0.4962 - val_accuracy: 0.7176\n",
      "Epoch 296/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4096 - accuracy: 0.8662 - val_loss: 0.4955 - val_accuracy: 0.7176\n",
      "Epoch 297/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4093 - accuracy: 0.8662 - val_loss: 0.4951 - val_accuracy: 0.7176\n",
      "Epoch 298/400\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4090 - accuracy: 0.8662 - val_loss: 0.4943 - val_accuracy: 0.7059\n",
      "Epoch 299/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4087 - accuracy: 0.8662 - val_loss: 0.4935 - val_accuracy: 0.7059\n",
      "Epoch 300/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4085 - accuracy: 0.8662 - val_loss: 0.4928 - val_accuracy: 0.7059\n",
      "Epoch 301/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4082 - accuracy: 0.8662 - val_loss: 0.4927 - val_accuracy: 0.7059\n",
      "Epoch 302/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4078 - accuracy: 0.8662 - val_loss: 0.4930 - val_accuracy: 0.7176\n",
      "Epoch 303/400\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4075 - accuracy: 0.8662 - val_loss: 0.4935 - val_accuracy: 0.7176\n",
      "Epoch 304/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4072 - accuracy: 0.8662 - val_loss: 0.4936 - val_accuracy: 0.7176\n",
      "Epoch 305/400\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4068 - accuracy: 0.8662 - val_loss: 0.4932 - val_accuracy: 0.7176\n",
      "Epoch 306/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4065 - accuracy: 0.8662 - val_loss: 0.4935 - val_accuracy: 0.7176\n",
      "Epoch 307/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4062 - accuracy: 0.8662 - val_loss: 0.4939 - val_accuracy: 0.7176\n",
      "Epoch 308/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4059 - accuracy: 0.8662 - val_loss: 0.4939 - val_accuracy: 0.7176\n",
      "Epoch 309/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4056 - accuracy: 0.8662 - val_loss: 0.4935 - val_accuracy: 0.7176\n",
      "Epoch 310/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4053 - accuracy: 0.8662 - val_loss: 0.4928 - val_accuracy: 0.7176\n",
      "Epoch 311/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4051 - accuracy: 0.8662 - val_loss: 0.4926 - val_accuracy: 0.7176\n",
      "Epoch 312/400\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4048 - accuracy: 0.8662 - val_loss: 0.4929 - val_accuracy: 0.7176\n",
      "Epoch 313/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4045 - accuracy: 0.8662 - val_loss: 0.4931 - val_accuracy: 0.7176\n",
      "Epoch 314/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4042 - accuracy: 0.8662 - val_loss: 0.4939 - val_accuracy: 0.7176\n",
      "Epoch 315/400\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.4039 - accuracy: 0.8662 - val_loss: 0.4946 - val_accuracy: 0.7176\n",
      "Epoch 316/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4036 - accuracy: 0.8662 - val_loss: 0.4957 - val_accuracy: 0.7412\n",
      "Epoch 317/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4034 - accuracy: 0.8662 - val_loss: 0.4970 - val_accuracy: 0.7412\n",
      "Epoch 318/400\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4034 - accuracy: 0.8662 - val_loss: 0.4976 - val_accuracy: 0.7412\n",
      "Epoch 319/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4031 - accuracy: 0.8662 - val_loss: 0.4968 - val_accuracy: 0.7412\n",
      "Epoch 320/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4028 - accuracy: 0.8662 - val_loss: 0.4953 - val_accuracy: 0.7412\n",
      "Epoch 321/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4025 - accuracy: 0.8662 - val_loss: 0.4947 - val_accuracy: 0.7412\n",
      "Epoch 322/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4021 - accuracy: 0.8662 - val_loss: 0.4944 - val_accuracy: 0.7412\n",
      "Epoch 323/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4017 - accuracy: 0.8662 - val_loss: 0.4932 - val_accuracy: 0.7176\n",
      "Epoch 324/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4013 - accuracy: 0.8662 - val_loss: 0.4916 - val_accuracy: 0.7176\n",
      "Epoch 325/400\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4013 - accuracy: 0.8662 - val_loss: 0.4907 - val_accuracy: 0.7176\n",
      "Epoch 326/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4009 - accuracy: 0.8662 - val_loss: 0.4907 - val_accuracy: 0.7176\n",
      "Epoch 327/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4006 - accuracy: 0.8662 - val_loss: 0.4910 - val_accuracy: 0.7176\n",
      "Epoch 328/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4003 - accuracy: 0.8662 - val_loss: 0.4916 - val_accuracy: 0.7176\n",
      "Epoch 329/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4000 - accuracy: 0.8662 - val_loss: 0.4926 - val_accuracy: 0.7294\n",
      "Epoch 330/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3996 - accuracy: 0.8662 - val_loss: 0.4945 - val_accuracy: 0.7412\n",
      "Epoch 331/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3996 - accuracy: 0.8662 - val_loss: 0.4972 - val_accuracy: 0.7412\n",
      "Epoch 332/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3997 - accuracy: 0.8662 - val_loss: 0.4992 - val_accuracy: 0.7412\n",
      "Epoch 333/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3997 - accuracy: 0.8662 - val_loss: 0.5000 - val_accuracy: 0.7412\n",
      "Epoch 334/400\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3996 - accuracy: 0.8662 - val_loss: 0.4993 - val_accuracy: 0.7412\n",
      "Epoch 335/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3991 - accuracy: 0.8662 - val_loss: 0.4974 - val_accuracy: 0.7412\n",
      "Epoch 336/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3988 - accuracy: 0.8662 - val_loss: 0.4960 - val_accuracy: 0.7412\n",
      "Epoch 337/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3981 - accuracy: 0.8662 - val_loss: 0.4950 - val_accuracy: 0.7412\n",
      "Epoch 338/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3978 - accuracy: 0.8662 - val_loss: 0.4938 - val_accuracy: 0.7412\n",
      "Epoch 339/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3975 - accuracy: 0.8662 - val_loss: 0.4930 - val_accuracy: 0.7412\n",
      "Epoch 340/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3970 - accuracy: 0.8662 - val_loss: 0.4925 - val_accuracy: 0.7412\n",
      "Epoch 341/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3968 - accuracy: 0.8662 - val_loss: 0.4916 - val_accuracy: 0.7294\n",
      "Epoch 342/400\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3964 - accuracy: 0.8662 - val_loss: 0.4911 - val_accuracy: 0.7294\n",
      "Epoch 343/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3961 - accuracy: 0.8662 - val_loss: 0.4905 - val_accuracy: 0.7294\n",
      "Epoch 344/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3958 - accuracy: 0.8662 - val_loss: 0.4899 - val_accuracy: 0.7294\n",
      "Epoch 345/400\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3955 - accuracy: 0.8662 - val_loss: 0.4894 - val_accuracy: 0.7176\n",
      "Epoch 346/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3952 - accuracy: 0.8662 - val_loss: 0.4889 - val_accuracy: 0.7176\n",
      "Epoch 347/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3950 - accuracy: 0.8726 - val_loss: 0.4888 - val_accuracy: 0.7176\n",
      "Epoch 348/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3947 - accuracy: 0.8726 - val_loss: 0.4894 - val_accuracy: 0.7294\n",
      "Epoch 349/400\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3944 - accuracy: 0.8662 - val_loss: 0.4900 - val_accuracy: 0.7294\n",
      "Epoch 350/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3941 - accuracy: 0.8662 - val_loss: 0.4898 - val_accuracy: 0.7294\n",
      "Epoch 351/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3938 - accuracy: 0.8662 - val_loss: 0.4887 - val_accuracy: 0.7294\n",
      "Epoch 352/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3935 - accuracy: 0.8726 - val_loss: 0.4870 - val_accuracy: 0.7294\n",
      "Epoch 353/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3932 - accuracy: 0.8726 - val_loss: 0.4857 - val_accuracy: 0.7294\n",
      "Epoch 354/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3933 - accuracy: 0.8790 - val_loss: 0.4850 - val_accuracy: 0.7294\n",
      "Epoch 355/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3929 - accuracy: 0.8790 - val_loss: 0.4850 - val_accuracy: 0.7294\n",
      "Epoch 356/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3926 - accuracy: 0.8790 - val_loss: 0.4854 - val_accuracy: 0.7294\n",
      "Epoch 357/400\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3921 - accuracy: 0.8790 - val_loss: 0.4867 - val_accuracy: 0.7294\n",
      "Epoch 358/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3918 - accuracy: 0.8726 - val_loss: 0.4887 - val_accuracy: 0.7294\n",
      "Epoch 359/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3915 - accuracy: 0.8662 - val_loss: 0.4906 - val_accuracy: 0.7294\n",
      "Epoch 360/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3913 - accuracy: 0.8662 - val_loss: 0.4921 - val_accuracy: 0.7412\n",
      "Epoch 361/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3912 - accuracy: 0.8662 - val_loss: 0.4931 - val_accuracy: 0.7412\n",
      "Epoch 362/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3911 - accuracy: 0.8662 - val_loss: 0.4934 - val_accuracy: 0.7412\n",
      "Epoch 363/400\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3909 - accuracy: 0.8662 - val_loss: 0.4929 - val_accuracy: 0.7412\n",
      "Epoch 364/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3905 - accuracy: 0.8662 - val_loss: 0.4917 - val_accuracy: 0.7412\n",
      "Epoch 365/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3901 - accuracy: 0.8662 - val_loss: 0.4902 - val_accuracy: 0.7294\n",
      "Epoch 366/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3897 - accuracy: 0.8662 - val_loss: 0.4887 - val_accuracy: 0.7294\n",
      "Epoch 367/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3894 - accuracy: 0.8662 - val_loss: 0.4873 - val_accuracy: 0.7294\n",
      "Epoch 368/400\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3890 - accuracy: 0.8726 - val_loss: 0.4861 - val_accuracy: 0.7294\n",
      "Epoch 369/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3890 - accuracy: 0.8726 - val_loss: 0.4851 - val_accuracy: 0.7294\n",
      "Epoch 370/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3887 - accuracy: 0.8790 - val_loss: 0.4847 - val_accuracy: 0.7294\n",
      "Epoch 371/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3885 - accuracy: 0.8790 - val_loss: 0.4846 - val_accuracy: 0.7294\n",
      "Epoch 372/400\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.3883 - accuracy: 0.8790 - val_loss: 0.4848 - val_accuracy: 0.7294\n",
      "Epoch 373/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3880 - accuracy: 0.8790 - val_loss: 0.4855 - val_accuracy: 0.7294\n",
      "Epoch 374/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3877 - accuracy: 0.8790 - val_loss: 0.4859 - val_accuracy: 0.7294\n",
      "Epoch 375/400\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3873 - accuracy: 0.8790 - val_loss: 0.4868 - val_accuracy: 0.7412\n",
      "Epoch 376/400\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.3871 - accuracy: 0.8726 - val_loss: 0.4881 - val_accuracy: 0.7294\n",
      "Epoch 377/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3869 - accuracy: 0.8726 - val_loss: 0.4891 - val_accuracy: 0.7294\n",
      "Epoch 378/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3868 - accuracy: 0.8726 - val_loss: 0.4895 - val_accuracy: 0.7294\n",
      "Epoch 379/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3865 - accuracy: 0.8662 - val_loss: 0.4896 - val_accuracy: 0.7412\n",
      "Epoch 380/400\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3862 - accuracy: 0.8662 - val_loss: 0.4904 - val_accuracy: 0.7412\n",
      "Epoch 381/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3861 - accuracy: 0.8662 - val_loss: 0.4912 - val_accuracy: 0.7412\n",
      "Epoch 382/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3860 - accuracy: 0.8662 - val_loss: 0.4912 - val_accuracy: 0.7412\n",
      "Epoch 383/400\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.3857 - accuracy: 0.8662 - val_loss: 0.4905 - val_accuracy: 0.7412\n",
      "Epoch 384/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3854 - accuracy: 0.8662 - val_loss: 0.4895 - val_accuracy: 0.7294\n",
      "Epoch 385/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3850 - accuracy: 0.8726 - val_loss: 0.4884 - val_accuracy: 0.7294\n",
      "Epoch 386/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3847 - accuracy: 0.8726 - val_loss: 0.4873 - val_accuracy: 0.7412\n",
      "Epoch 387/400\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3844 - accuracy: 0.8726 - val_loss: 0.4863 - val_accuracy: 0.7412\n",
      "Epoch 388/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3841 - accuracy: 0.8726 - val_loss: 0.4859 - val_accuracy: 0.7412\n",
      "Epoch 389/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3839 - accuracy: 0.8790 - val_loss: 0.4861 - val_accuracy: 0.7412\n",
      "Epoch 390/400\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3836 - accuracy: 0.8790 - val_loss: 0.4872 - val_accuracy: 0.7412\n",
      "Epoch 391/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3833 - accuracy: 0.8726 - val_loss: 0.4889 - val_accuracy: 0.7294\n",
      "Epoch 392/400\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.3834 - accuracy: 0.8726 - val_loss: 0.4897 - val_accuracy: 0.7294\n",
      "Epoch 393/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3830 - accuracy: 0.8726 - val_loss: 0.4889 - val_accuracy: 0.7294\n",
      "Epoch 394/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3827 - accuracy: 0.8726 - val_loss: 0.4877 - val_accuracy: 0.7412\n",
      "Epoch 395/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3824 - accuracy: 0.8726 - val_loss: 0.4862 - val_accuracy: 0.7412\n",
      "Epoch 396/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3820 - accuracy: 0.8790 - val_loss: 0.4843 - val_accuracy: 0.7294\n",
      "Epoch 397/400\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3817 - accuracy: 0.8790 - val_loss: 0.4819 - val_accuracy: 0.7294\n",
      "Epoch 398/400\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3818 - accuracy: 0.8790 - val_loss: 0.4795 - val_accuracy: 0.7412\n",
      "Epoch 399/400\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3821 - accuracy: 0.8790 - val_loss: 0.4778 - val_accuracy: 0.7412\n",
      "Epoch 400/400\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3825 - accuracy: 0.8790 - val_loss: 0.4770 - val_accuracy: 0.7294\n"
     ]
    }
   ],
   "source": [
    "model_heart = Sequential()\n",
    "model_heart.add(Dense(7, input_dim=13, activation='relu'))\n",
    "model_heart.add(Dense(4, activation='relu'))\n",
    "model_heart.add(Dense(1, activation='sigmoid'))\n",
    "epochs = 400\n",
    "\n",
    "model_heart.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.01, patience=8, verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "model_heart.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_without_early_heart=model_heart.fit(X_train, y_train, validation_split=0.35, epochs=epochs, batch_size=128)\n",
    "\n",
    "SEQ_score_heart = model_without_early_heart.history.get(\"val_accuracy\")[epochs-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names_of_approaches = [\"SVM\", \"Decision Tree\", \"Random Forest\", \"KNeighbors\", \"Sequential\"]\n",
    "scores_of_approaches = [SVC_score, DTC_score, RFC_score, KN_score, SEQ_score]\n",
    "scores_of_approaches_heart = [SVC_score_heart, DTC_score_heart, RFC_score_heart, KN_score_heart, SEQ_score]\n",
    "\n",
    "plt.title(\"Forecast Efficiency\", fontsize = 16, fontweight = \"bold\")\n",
    "plt.stackplot(names_of_approaches, scores_of_approaches, scores_of_approaches_heart)\n",
    "plt.legend(labels = [\"Stars\", \"Heart\"], loc = \"upper left\")\n",
    "plt.xticks(names_of_approaches, rotation = 45)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Name\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_homeworks_sdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b77d8a2effab1ba25922da0c4c81ff0352d3c51593844af95c490f6561b2d93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
